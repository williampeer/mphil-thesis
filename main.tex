%%%%%%%%%%%%%%%%%%%%%%%%
% Sample use of the infthesis class to prepare an MSc thesis.
% This can be used as a template to produce your own thesis.
% Date: June 2019
%
%
% The first line specifies style options for taught MSc.
% You should add a final option specifying your degree.
% *Do not* change or add any other options.
%
% So, pick one of the following:
% \documentclass[msc,deptreport,adi]{infthesis}     % Adv Design Inf
% \documentclass[msc,deptreport,ai]{infthesis}      % AI
% \documentclass[msc,deptreport,cogsci]{infthesis}  % Cognitive Sci
% \documentclass[msc,deptreport,cs]{infthesis}      % Computer Sci
% \documentclass[msc,deptreport,cyber]{infthesis}   % Cyber Sec
% \documentclass[msc,deptreport,datasci]{infthesis} % Data Sci
% \documentclass[msc,deptreport,di]{infthesis}      % Design Inf
% \documentclass[msc,deptreport,inf]{infthesis}     % Informatics
%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[mphil,deptreport,ianc]{infthesis} % Do not change except to add your degree (see above).
\usepackage[final]{pdfpages}
\usepackage{siunitx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=gray,
    urlcolor=blue,
    pdftitle={Spiking neural network model inference},
    pdfpagemode=FullScreen,
    }

% \urlstyle{same}

\begin{document}
\begin{preliminary}

\title{Spiking neural network model construction, inference, analysis and applications}

\author{William Peer Berg}

%   Looking to machine learning and the success in applying gradient-based optimisation to tackle high-dimensional modelling problems, we here investigate the potential of applying this to the class of spiking neural network models. Further, we look at statistical assessment methods, and application of outlined methodology both to synthetically generated as well as biologically recorded in vivo data.
%   In sum, our results show that models may capture higher-order statistics of recorded nuclei, and that using some parallelisation tricks, we may decrease the computational cost to some extent. However, the lower bound on computational complexity still makes it challenging to apply the outlined methodology to very rich (i.e. high number of recorded nuclei) data sets.

% ======== Should be rewritten after main text completed. ============
% Computational models are used to try and explain observed data.
% By implementing models that replicate observed data, the model itself may explain why and how the modelled system behaves as it does, and what its function is.
% However, in order to be able to propose that the behaviour and functioning indeed arises due to mechanisms and dynamics similar to those in the model, the parallel between model properties and dynamics and in our case the brain and recorded nuclei needs to be firmly established.
% These are usually established by design, and verified by statistical comparisons over different metrics.
% However, to this day, constructing biologically interpretable and realistic models remains highly challenging, with no robust data driven approach working for other than very small networks.
% While there is significant ongoing research efforts in bridging data driven approaches with computational modelling, this remains an unsolved task for neuron-level resolution spiking neural networks with a scalable methodology.
% Currently, the most promising algorithm involves approximate Bayesian computation by training a deep neural network to approximate the posterior of the output (here spike trains) over the model parameters as the prior, and using simulation-based inference in order to generate samples, i.e. to simulate spike trains given a prior.
% As the reader may note from this, the approach does not scale well - but it does maintain a full posterior over the parameters, as they are sampled altogether.
% % SNNs biologically realistic models. 
% Do we have the tools for SNN parameter inference, instead of rigorous and tedious hand-engineering?
% ML advances: GBO.
% SNNs differ from RNNs in a fundamental/crucial way. Show via parameter landscape and loss functions.
% However, can find frontier(s).
% SNN variants and applicability of GBO (only NLIF thus far, but still local minima).
% To some extent GIF; frontier.
% Same for GLIF versions??
% Input scheme. Param landscape plots.
% Loss functions.
% PyTorch (+batching)

% ABC, (EA; e.g. NGO)

% Albeit GT off for the above, comparison wrt NMF.
% Hand-engineered case and full analysis with NMF and LDA.

% In conclusion...

% BONUS (if time): Fitting to sleep data, with small sleep regul PPT/LDT lit rev first?

% BONUS2: Append Izhikevich paper and place in context.

% BONUS3: ?


    % Neuroscience -> ML.
    % Now: ML -> Neuroscience?
    % SNNs hard. However, since DNNs work so well: SNNs as DL-case?
    % Tricks \& hacks to allow for GBO for SNNs, too!
    % Modern PyTorch-implementation - much performance, such high-level library.
    % Batching is nice.
    % Model classes. Performance over experiments with synthethic data \& GT, and on synthetic general data, and on sleep data.
    % Also probabilistic model, which lends itself better to optim by NLL, rather than frd or vrd.
    % Vrd worse probably due to stochasticity obscuring rather than refining error gradient signal w metric.
    % Param landscapes GBO
    % SBI using DNN to estimate posterior over parameters: Intractable for neuron-level (?). Compared with pop-level classes. Finding?
    
        % More data, and higher res. data available. 
    % In today's modern society, unfathomable amounts of data are generated every split second. 
     % Often experts are required to make an effort at hand-engineering models for particular data. 
        % The key ingredient to enabling this advance has been hardware-improvements, which in turn have enabled using backpropagation for large network-models, which have been shown to be universal function approximators. In other words, by scaling up the network size, one may approximate a correspondingly more complex function and data set.
    % At the very core of the inference procedure for this model class, we find gradient based optimisation (GBO).
    % In this work, I look at whether this may be leveraged for SNNs.
    % As modeling within computational neuroscience is a tedious endeavour, particularly of more biologically realistic models that entrain biological aspects in their parameters, it is natural to look at whether the successful approach for the highly complex tasks that have been solved by DL may be leveraged for SNNs.
        % , rather than the current state-of-the-art, which is simulation Based inference using approximate Bayesian computation - in fact with a DNN to estimate the posterior over the simulator (here SNN) parameters.
    % The probabilistic SNNs are compared to a published approach in which making a Bernoulli assumption over the spike history enables using the negative log likelihood for optimisation over population-level models. 
    % In this work we test all approaches for both population-level and neuron-level models.
        % NMF ensembles and geodesic dist. - spike train correlations better with SNN models than with GLMs? if not, no inductive bias / little gain with spike train as signal.
            % NLIF shows high sensitivity to initial parametrisation. also suggests ... different nature spatiotemporal signal of spike train.
    % may be fruitful to use synapse model in conjunction with the above, but overall the results suggest that under the current formulation
    
\abstract{
    Computational models have long been used as hypotheses to illuminate and unravel aspects of neural functioning, with seminal works including models such as the Hodgkin-Huxley model,
    % Models are themselves hypotheses of the system we wish to study, and may both explain functional aspects about recorded areas, and 
    which also exemplifies that model hypotheses may be tested with in vivo or vitro experiments.
    However, designing high-dimensional biologically realistic models can be an arduous endeavour, and may require hand-engineering for each setting.
    Automating this arduous process could potentially greatly accelerate computational research within neuroscience.
    Therefore, statistical approaches have been used to try and aid in such modelling, with the current state-of-the-art being based on Bayesian approximation.
    However, this approach does not scale well with growing resolution, i.e. a growing number of neurons.
    In the current data driven era, where deep learning is the prevalent state-of-the-art within machine learning, we investigate whether its key ingredient; gradient based optimisation (GBO) may be leveraged for spiking neural network (SNN) model inference.
    To this end we implement a modular gradient based optimisation framework on top of PyTorch, a modern ML Python library, and test to what extent GBO may be used for SNN inference, particularly because this approach scales well with network size.
    GBO is tested for a rate based loss metric, as well as for the van Rossum distance, which also emphasised the timing of spiking, over different classes of SNNs, including generalised versions of leaky integrate-and-fire, non-leaky integrate-and-fire, and probabilistic spiking models, with some extensions to subthreshold synaptic current models, which may enable for more continuous signals.
    The results show that due to the temporal nature of SNNs, the van Rossum distance metric and emphasis on the precise timing of spiking obscures the gradient signal due to the spike train stochasticity and resulting variability.
    Further, the parameter landscapes formed by the rate-based metric contains no saddle point, and at best a frontier as the global minima.
    These observations and results are reflected in the inferred parameters when compared to the ground-truth models for the synthetically generated data.
    As for the spike trains themselves produced by the models, we wished to test whether higher-order statistics had been captured to a greater extent yet in the SNNs, as when compared to generalised linear models as the baseline.
    When factorising the spike trains by using non-negative matrix factorisation (NMF), the resulting ensembles reveal similar functional ensembles for the synthetic data - however, the extent to which the geodesic similarity is better captured in the SNNs is very limited, and may simply be due to inductive bias.
    For the optimisation procedure itself, DNNs are known to be highly sensitive to their initial parametrisations wrt model performance and optimisation convergence.
    SNNs are no different, and seem to be even more sensitive to this, likely due to the temporal nature of the model class, in which the initial state may exert a significant effect on the modelled spike trains and futue model state.
    Overall, the results suggest that with GBO we are almost guaranteed to converge towards a local minima due to ambiguity of the loss signal, particularly when the input signal is unknown.
    Even when the input is known, such as for the sine-modulated white noise input, it is hard to correctly disentangle the signals in such a way that metrics emphasising the precise timing of spiking may be used.
    As such, it would be interesting to study closed-form non-leaky models in future work, potentially in conjunction with different synapse model, and different loss metrics that may form a better basis for gradient signal backpropagation.
}

\maketitle

\section*{Acknowledgements}

I would like to thank my supervisory team for offering their advice throughout my research.
I would also like to thank members of my lab group whom offered invaluable support and advice on everything from my project work to mastering stress and finding a flat in Edinburgh.
I also sincerely appreciated input relating to mathematics and philosophy from two special friends in Ediburgh whom are outside of my lab group - you know who you are.
Lastly, I would like to sincerely thank the head of our collaborative lab at the University of Strathclyde whom gave me access to in vivo data from the brainstem that was analysed wrt sleep regulation. It was truly inspiring and exciting to be able to see their experimental lab from early on in my project work.

% Nina.
% Matthias.
% Shuzo.
% Luke.
% Patricia.
% Etienne.

\tableofcontents
\end{preliminary}


\chapter{Introduction}


% The report then contains a bibliography and any appendices, which may go beyond
% page~40. The appendices are only for any supporting material that's important to
% go on record. However, you cannot assume markers of dissertations will read them.

% Citations (such as \cite{P1} or \cite{P2}) can be generated using
% \texttt{BibTeX}. For more advanced usage, the \texttt{natbib} package is
% recommended. You could also consider the newer \texttt{biblatex} system.

% You may not change the dissertation format (e.g., reduce the font
% size, change the margins, or reduce the line spacing from the default
% 1.5 spacing). Over length or incorrectly-formatted dissertations will
% not be accepted and you would have to modify your dissertation and
% resubmit.  You cannot assume we will check your submission before the
% final deadline and if it requires resubmission after the deadline to
% conform to the page and style requirements you will be subject to the
% usual late penalties based on your final submission time.

% MPhil:
% One who strives towards a goal to prove themselves, may suffer along the way.
% One who strives towards a goal as a means of self-expression, may enjoy the way.

% \section{A synthesis from the brain to deep learning and back again?}

Spiking neural networks (SNNs) contain variables that represent biological properties such as the membrane potential, cell membrane time constant relating to the type of cell and ion channels, and potentially other neurotransmitter dynamics. 
Their definition results in dynamics resembling that of biological neurons, including the release of action potentials upon reaching a certain membrane potential.
Due largely to the biological plausibility of this class of neural network models, they are appealing to study in computational neuroscience, as they maintain these biological parallels to an extent which the feed-forward networks in deep learning do not.
However, SNN inference research is highly limited and faces a number of challenges due to the temporal nature of SNNs and the associated increased complexity, stemming both from state-dependence, resulting temporal stochasticity, and the (high) parameter dimensionality.
On the other hand, machine learning has seen a recent surge of interest due to the large success of employing gradient based optimisation for deep neural networks (DNNs), in part unlocked by the increase in available computational power and data. 
Numerous fields have replaced their state-of-the-art models with surrogate deep neural network models that capture the structure in the data.
Thus, it is only natural to ask whether the foundational methodology of gradient descent may be applied in other fields.
I find this particularly interesting for neuroscience, which is after all the origin of deep neural networks in machine learning, loosely inspired in the networks of coupled nodes, thought to represent neurons.
Some of the ongoing research in computational neuroscience is now in fact focussed on whether deep learning may inspire model inference within a domain closer to its origin; namely for biologically plausible neural network models.
However, these approaches either greatly constrain model fitting by making statistical assumptions and inferring models with a low number of nodes, or use DNNs to try and learn the relation between SNN parameters and spike outputs.
This work is focussed mainly on inference of SNN models using gradient based optimisation, with the aim of accelerating research towards a scalable inference algorithm for this model class of biologically plausible and interpretable neural networks.
As more and more data, and data of a higher resolution, is becoming available from neural brain recordings, the potential benefits of developing a scalable approach for model inference only grows.
We here revisit the state-of-the art for inferring SNN models using both surrogate gradient based optimisation and some of the most prominent spike metrics, as well as deep neural network amortized learning and approximate Bayesian computation (ABC), and compare how these approaches may be employed for SNN inference. Further, we hypothesise that recent ML techniques may be leveraged for successful and efficient model inference using surrogate gradient descent.
We find that while ABC may be successful for population-level models, as has been shown recently in the literature, but that its algorithmic and computational complexity and cost thereof limits the methodology to models with a low number of nodes.
However, using a surrogate gradient approach, we find that model inference for larger networks is made possible, albeit for local minima, and not for consistently retrieving the ground-truth values as tested with synthetic data generation. 
% Since it would greatly help with SNN model construction if automatic inference based on various data became possible and available to the research community, this has been one of my primary research goals.
We perform model inference using leaky integrate-and-fire (LIF) models, generalised leaky integrate-and-fire (GLIF) models, and stochastic general integrate-and-fire models (SGIF) using (both) synthetic (and biological) target data, and demonstrate that higher order statistics may be captured (to some extent) even when performing neuron-level model inference over a mixed neuron-type network. The results are verified by comparing geodesic similarities of inferred model spike train outputs with predicted spike trains produced by fitting generalised linear models (GLMs) to the target data.
Our findings there illuminate that the inference procedure is in fact capable of capturing the higher-order statistics to the same extent as a coupled Poisson GLM.
% This may advance research on SNN inference, and also demonstrates how modern ML frameworks as well as techniques may be leveraged to this end.
However, we observe that it is highly problematic to aim for retrieving ground-truth parameters in SNN models when using gradient based optimisation. 
This is largely due to the issue of constructing a well-defined loss metric.
As our best results were produced when using a binned rate-based metric, the resulting error landscape for the parameters often does not contain a single point, or even a basin, for the global minima.
Instead, this is a region or space in which the gradient may wander arbitrarily when performing GBO. As such, we will likely only end up within this space, but the global minimum is not retrievable, or well-defined, when considering this through the lens of the rate-based metric.
% A large part of the work presented in this thesis will revolve around why SNNs do not lend themselves as well as feedforward neural networks as within deep learning, even though they also implement a type of recurrence using recurrent units with memory, in addition to potential recurrent connectedness.
To try and give a more general an intuition about why this is; if we think about deep neural networks (DNNs) in the machine learning (ML) domain - these may approximate arbitrary data well, given that they are universal function approximators. 
However, this requires that the data is of such a structure that it may be represented and captured by a function which the network learns by spatial representation and transformation.
However, in the domain of SNNs, we introduce several new crucial properties, which results in distinct model dynamics. Each neuron now has a state, which depends on its previous state, and the behaviour is modelled typically as a system of ordinary differential equations (ODEs), containing parameters representing the membrane potential, membrane time constant relating to a refractory period, transmitter interaction, and other biological properties. 
While this makes the model biologically interpretable with direct parallels between parameters and biological and cellular counterparts, it also changes the entire system's behaviour.
The system now has activity inedpendently of input perturbation. 
The transformation of an input signal is no longer deterministic in the sense that it will result in one given output given a set of initial model parameters - it now depends on the current system's state, which again depends on the previous state.
As such, gradients calculated in a manner similar to that of for DNNs in ML will also depend on the system's and neurons' state, greatly increasing complexity.
In a way, one may still maintain the parallel to backpropagation through time for DNNs. 
However, there is a crucial difference in that a neuron's output is now of a much more binary nature; a spike or no spike, and in that the time series evolution of spikes, whilst potentially encoding much more information, now is a series of spike evolving over time, and thus the target signal cannot be regarded in the same way as in DNNs.
This being said, I believe like noted by other authors such as \cite{Sindaci2018} that researchers are looking for a connection between the field of machine learning and computational neuroscience, the former having had its subfield of DNNs created based on inspiration from the brain, but now in a reverse way in which we may leverage the advances from the domain of ML in model construction.
Even though we are not there yet, I believe the search for a connection here may both be highly beneficial for the community, and spawn a new sub-field of dynamical systems modelling using inference in computational neuroscience.

% My main findings, although included throughout in the rest of my thesis, may be summarised as following:
Although SNN inference certainly would be of great value to the field, there are some key issues that I have identified and tried to address in my work, some of which are included in the subsections below.

\subsection*{SNNs are hard to optimise}
% I am starting to believe that they’re in fact not possible to optimise, due to the parameter landscape, which results from the temporal nature of the target signal.
In DNNs, we only consider approximating a spatial transformation over the input data. Not only does this allow for higher parallelisation of the training algorithm, but it may also greatly constrain the parameter landscape in the sense that it does not depend on the past activity of the network itself, which vastly increases both the complexity and stochasticity of parameter inference. 
One way to ameliorate this is to eliminate some of the stochasticity by making the neurons non-leaky. Since we may computationally set the system’s initial conditions and random seed, the following deterministic computations have little to no fluctuations, and with no leakage, gradient propagation may be computed exactly according to the error signal.

However, there should exist solutions to the set of coupled ODEs that maximise a given loss metric. The question would then be how large the solution space is, and to what extent the different regions are reachable. Furthermore, we are not guaranteed that the solution(s) given by our metric captures the precise temporal dynamics. In fact, it is limited how much of this information can be incorporated into and made relevant by the loss metric.
Nevertheless, since BPTT and optimisation for DNNs within DL has been shown to be able to be able to infer highly complex data sets, it is our goal to study the extent to which a similar methodology may be applied in SNNs, to evaluate the performance of this novel algorithmic design, and to illuminate where work that bears potential may lie.


\subsection*{The binary nature of spiking}
One way of addressing this issue is by incorporating the signal below spike-threshold, i.e. the membrane potential, and have this generate a signal inside of an active zone, such as when the potential is above 0. This is not entirely biologically unrealistic, …, and it offers the advantage of a continuous signal centred around potential (no pun intended) spike pulses, or action potentials. Note that it also provides a gradient signal for spike generation even when the potential may be below threshold, still making the excitation visible to the optimiser.
This bears some resemblance to loss metrics such as the van Rossum distance, in which each spike pulse is convolved with an exponentially decaying kernel, such that we may operate over a smoother signal. However, this signal transformation transforms each spike pulse the same, rendering the sub-threshold gating synapse model a more fine-grained candidate, since it contains richer differentiable information, and not just spike-time information.

Temporal kernel no new information - but transforms signal such that it contains a trace which may be more easily optimised. Think about aligning two waveforms, rather than two short pulses, where the information gain is only visible in the step which matches them.


\subsection*{The parameter landscape contains a frontier of local minima (at best)}
Even when addressing the issues above with the proposed approaches, and even if fixing all model parameters but the weight matrices, there are multiple weight configurations that may result in very similar behaviour and outputs.
As noted by other researchers in the DNN as well as SNN literature (...) the initial configuration has a large effect on the resulting inferred weights and/or model. This holds particularly true for SNNs, where the previous model state also has an effect on the future state, and the model neurons may be brought to exert different modes of behaviour, depending on the previous and current input and state.

\subsection*{Hand-engineered models may be qualitatively similar to local minima}
By hand-engineering models it is possible to attain qualitative patterns and behaviour, which has been a common practice in the field of comp neuro - however, this may not be satisfactory for attaining higher-order statistics, and requires a large amount of resources. Therefore, model inference may likely be a better option. 
The above however indicates that GLMs may be the best we can do in this regard (wrt spike trains), and demonstrates a need for advancing SNN inference.
The way going forward?
Amortised learning provides a posterior over the parameters.
However, it quickly becomes intractable due to the high complexity of models.
Surrogate or exact gradient approaches are difficult due to aforementioned issues, and most likely do not settle into a stable configuration, as well as stay in planes of local minima.
These findings suggest that the best we can currently do is using approximate Bayesian approaches, which not only lets us infer a posterior over all of the parameters, but also provides a measure of certainty around the parameters.

\subsection*{In sum}

GBO may be leveraged for a more scalable inference procedure.
However, "true" parameters are not well-defined when considering spike trains as the target signal, with unknown inputs, and using LIF-type of models.
When considering either a more low-dimensional target signal, such as a linear combination of sine-modulated noise, or when making stronger assumptions about the input (as made be done in vitro by stimulation), we may obtain better model fits.
However, it is crucial to be able to incorporate information about the timing of spiking in the loss metric in order to traverse a more well-defined parameter-landscape.
The error becomes more trivial to calculate for a lower-dimensional target signal, as correspondingly considerably constrains the parameter landscape.

When it comes to more large-scale networks with spike trains as the target signal, networks of a single layer are seemingly too "free" in their parameter landscapes and stochastic nature in order to be able to form a well-defined optimisation problem.
However, we may infer models that yet capture higher-order statistics.
These may thus be used to probe functional characteristics and dynamics of the site which the spike data stems from, or as a starting point for model construction.
In fact, the geodesic similarity of NMF ensembles is higher for some inferred models than for our baseline coupled Poisson GLM, suggesting either that the GBO inference procedure may capture more of the spike statistics, or potentially that we have done so by the inductive bias induced by optimising a model as the same class as the target data.

When testing this hypothesis on biological data, however, we find that ...
indeed we may...

This work surveys the literature on GBO related to SNNs, demonstrates a way in which GBO may be implemented and performed scalably for the SNN model class in-place, and tests the limitations of the procedure, as well as connects it with and compares it with the state-of-the-art for a specific stochastic model type, and for a prominent simulation-based inference by approximate Bayesian computation approach.


% =======================================================
% =======================================================
\chapter{Background: Biologically plausible computational modeling}\label{chpt:background}

% Mainly surrogate gradient descent, but also some conversion approaches.
% Spiking neural networks
In disciplines pertaining to biology, maintaining a more direct link in hypothesised models by incorporating a greater level of detail, may allow for correspondingly more illuminating findings.
However, this comes at the cost of increased complexity, often resulting in that construction of such models by sheer hand-engineering is too time-consuming, as the combinatorial expansion of possible solutions to model parametrisations quickly rises to one above where manual search is out of the question.
Of course, hand-engineering often involves both expert system and domain knowledge for constraining the semi-manual parameter search, and model proposals, and some type of programmatic model search.
However, inference of spiking neural networks remains a largely unsolved task.
With the recent advent of widespread success of deep learning models for arbitrary data sets, we here revisit applying a similar inference procedure for automating inference of biologically plausible models, too - namely spiking neural networks.

There are multiple model definitions and topologies one may consider when studying neural network architectures, which in turn affects network behaviour and inference.
From the original perceptron \cite{Rosenblatt1956} to the Hodgkin-Huxley (HH) model \cite{HH1952}, and from single-neuron to multi-layer networks.
On the one hand, the HH model is complex enough that fitting the parameters of a single node using electrophysiological data requires a significant amount of computational resources, which in turn allows the model to accurately replicate biological membrane potentials and spikes.
% The \cite{HH1952} 1952 paper more or less hand designing the ODE system in a way that generalises well to describe neuronal and cellular electrophysiology.
Perceptrons, on the other hand, are far less costly to train and fit, requiring only straightforward matrix multiplications for weight inference, with the nodes' values being either 0 or 1 - however, the behaviour these networks may exert is highly limited and implausible.
A common mentioned example to illustrate this is that a two-layer network consisting of perceptrons cannot learn or perform the XOR-task, as hidden layers are required to perform this non-linear separation.
More complex models, however, may incorporate non-linearities also in single nodes.

% Supervised, semi-supervised - different tasks.

In this thesis, we are focussed on inference of the more complex class of spiking neural networks, which maintain a level of biological plausibility.
There are different approaches for tackling the associated increased complexity, as well as binary nature of spiking itself, resulting in non-linear changes of the internal neuronal variables.
On the one hand the temporal unfolding of the internal state and their significantly exerted effect on activity requires loss metrics that are adapted to also measure this effect, and on the other hand the limited information when modeling using biological data poses a constraint on the extent to which timing may be emphasised in metrics.
Further, the explicit encoding scheme adapted in SNNs is traditionally a type of rate-based encoding, but this does not make use of the rich information available in the timing of spiking.
Population-encoding is another type of approach which may make readouts and learning more stable in a network, but also imposes a significant information bottleneck on the amount of information that may be processed and encoded by the network.
It is likely that the brain employs an encoding scheme in which the rich information contained in the timing of spikes is relevant. 
However, as we shall see throughout the work studied in this thesis, it is not possible to do so simply by means of a global gradient descent procedure, which seems to be neither sufficient for precise timing emphasis, nor is biologically plausible.
This being said, the scenarios we have tested only include conditions where the input is unknown, and only the probabilistic nature of it is incorporated.
We also only study using error back-propagation by using rate based loss metrics, which was found to be the best-performing procedure when doing gradient descent when fitting to spike train data.
Surprisingly, when fitting to a lower-dimensional signal, such as a linear combination of a sum of sine-modulated white noise inputs, we may retrieve a model configuration that almost perfectly learns the input to output mapping, by extending the work of \cite{Huh2017}.
There are a number of key aspects for why this works, including greatly constraining the parameter landscape by using a lower-dimensional target signal, by using a well-defined, non-noisy signal, and by using a means of gradient-descent that in fact may be said to approximate a type of STDP, as we are using sub-threshold synapse currents instead of a surrogate over the spike signals - which allows for distinct sub-threshold signals instead of a uniform signal surrounding each spike.

% However, principles in which these may be learned are non-trivial to devise, or are non-applicable to noisy data with unkown input.
% TODO: Update. Also make scope and goal of modeling bio. data much clearer! (gjennomsyre tekst)

% Various SNN models
For further reading on different learning procedures currently studied in the context of SNNs, we refer the reader to \cite{Taherkhani2020}.

% Mackelab meso paper: Only rate based at first.
% NN 2020 review: Loses out on temporal information.
% NMF: Spatiotemporal decomposition - thus rate based should not be able to infer spatial activation coefficients. However, encode populations into SGD, and rate based may? 
% Topological averaging.
% Search using micro-model directly? Data set requirements? Spike trains - rate per node and binom dist.?


\section{Fundamental differences between SNNs and spike train PDF models}

A spike train can be modeled by drawing from a probability distribution $P(X; \lambda)$.
This is perhaps the simplest baseline model one may construct, also assuming complete independence between neurons in a data set.
However, this type of model may quickly break down when not assuming dependence between neurons.
It may also be updated assuming independence, resulting in a more complex probability distribution function (PDF).
A more robust baseline model would be a generalised linear model, in which a spike train may be modelled by assuming a Poisson distribution as the spike response model for each neuron (as spike trains are of a Poissonian nature), with dependence between nodes, coupling them over either the spikes or modeled spike responses - we fit GLMs of both natures in this work as baseline models for comparison with inferred SNN models.
GLMs may capture spike correlations and rates well, but not exert different modes of behaviour.
I.e. it is a spatio-temporal model with the temporal signatures being limited to static functions over spike-responses, which may be somewhat affected by input, but not fundamentally change its dynamics and behaviour, as opposed to in SNNs, which may exert distinct modes of behaviour \cite{Izhikevich2004}.
The golden standard should as such be to infer models that also capture different modes of behaviour, thus also the dynamics that are at play and may be exerted under different input conditions.
This may be extremely hard to capture using current inference methodologies, and has to the best of my knowledge not been successfully attained by the research community.
In this work, we use non-negative matrix factorisation (NMF), described in more detail later in this chapter, to assess the functional ensembles captured by models, as it has shown to be able to capture ensembles sufficient for predicting future brain state almost as well as the raw neuron-signals themselves, suggesting that the factorised modules indeed as a suitable representation of potentially functional ensembles, and may well pertain to the dynamics observed in the data.
Further, different input and stimulus conditions is in fact something that may be tested both synthetically and in vitro or vivo by stimulation e.g. patch-clamp tissue stimulation or optogenetically.

% Typical leaky integrate-and-fire SNN neuron model definition:

% \begin{equation}
%     \tau_m \frac{dv}{dt} = E_L - v(t) + I(t)
% \end{equation}

% where $v(t)$ is the membrane potential, and $I(t)$ is the synaptic current, usually summing over some stimulus as well as neuronal synaptic inputs and associated weights, potentially summed through a transfer function that bounds the current,

% \begin{equation}
%     I = \sum_{i,j}^{N} W_{i,j} s_i + I_{ext}
% \end{equation}

% where $W_{i,j}$ is the synaptic weight from neuron $i$ to neuron $j$, and $s_i$ is the pre-synaptic current from neuron $i$.

\section{The link to recurrent neural networks (RNNs)}

As mentioned previously, SNNs are systems in which each node is modeled by a set of ordinary differential equations (ODEs), where each variable or parameter may have a more or less direct biological parallel, such as representing the rest potential $E_L$, the spike threshold $V$, and other properties.
Together, these result in a model whose membrane potential $v_t$ has a temporal trajectory similar to that of biological neurons.
The main idea is that under the right conditions, SNNs may mimic biology so closely, that we can in fact use them to study what might be going on in the modeled brain area; the model becomes the hypothesis.
The stronger the link between the model and the brain area, the stronger the hypothesis, and associated predictions made by perturbing and probing the model under specific conditions.

In RNNs, however, the criterion of each node or neuron being a system of ODEs that maintain biological parallels is relaxed or no longer valid. 
Each node may simply be one value that is some sum over synaptic input that has been transformed by a transfer function, or it is more commonly some slightly more complex unit such as the gated recurrent unit (GRU) \cite{Bengio2013b, Chung2015a} or long short-term memory (LSTM) unit, engineered for application in the domain of machine learning \cite{Hochreiter1997, Schmidhuber2014}.
When dealing with nodes that are commonly used in machine learning, the temporal dependence is more limited than in more complex SNNs and neuronal ODE systems.
Further, RNNs are usually trained on tasks in which the input is known, and is some static spatial transformation of inputs to outputs, which forms the data set.
In other words, the data modeled is usually non-noisy, and of a more straightforward nature than (decoded) brain recordings such as spike trains.
These aspects make RNNs far more suitable for chaining the error gradients backwards in time, which may be done over each time-step for each error gradient by a modification to the backprop-algorithm by chaining the error gradients, known as back-propagation through time (BPTT) \cite{Rumelhart1986}.
Note also that RNNs typically deal with continuous, smooth signals in each node, whereas SNNs incorporate non-linear value-changes upon binary spike-events, which also greatly complicates training, let alone inference of model parameters.
\cite{Neftci2019} describe the connection between RNNs and SNNs well, including the efforts that have been made to employ (surrogate) gradient descent for spiking neural networks.
In order to give an intuition about the differences, between SNNs and RNNs, the definition of the GRU unit is included below, and may be written as,

% RNN transfer function, unit as GRU versus dv/dt etc.
\begin{equation}
    \textbf{h}_t^j = \textit{f}(W^{i,j}\textbf{h}_t^i) + \sum_{i=1}^L g^{i,j} U^{i,j}\textbf{h}_{t-1}^i
\end{equation}

where $h$ is the hidden units, $W$ the weights, and $g$ a gating function,

\begin{equation}
    g^{i,j} = \sigma ( \textbf{w}_g^{i,j}\textbf{h}_t^{i} + \textbf{u}_g^{i,j}\textbf{h}_{t-1}^{*})
\end{equation}

basically treating the network instead of a conventional RNN which may be regarded as a (POMDP) process as a process in which each hidden layer has gated recurrent connections determined by a reset gate function, which allows the ignoring of previous hidden states by transition through the reset gate function, whose weights may also be updated by the same methodology;
estimating a probability distribution over sequences, factorising the probability of the sequences, and training an RNN by means of negative log-likelihood minimisation of the training sequences:

\begin{equation}
    p(x_1, ..., x_N) = p(x_1)p(x_2|x_1) ... p(x_N|x_1,...,x_{N-1}), \\
    p(x_N|x_1,...,x_{N-1}) = g(\textbf{h}_t)
\end{equation}

Note that this bears resemblance to SNNs, and is bridged particularly by the probabilistic model version of \cite{Rene2020}, which we have based a spike time and gradient compatible model implementation on in section \ref{microGIF}.
There, a spike train is assumed to be a Bernoulli or Poisson distributed spike history, which may be estimated with the ODE system and its recurrent connections.
However, connections in a GRU system are optimised in a more straightforward manner, with the other parameters and internal dynamics instead being what decides on the "gating".

While typically only the weights are inferred in ML models using some gradient based optimisation algorithm, there exists work in which SNNs are transformed such that they are more compatible with BPTT, including probabilistic models in which each node projects a probability of spiking for each time step or interval, and models in which synaptic signals are made continuous and smooth, either by constructing a surrogate gradient signal such as over the membrane potential, or by modeling the signals as a function over the sub-threshold membrane potential, such as for instance when it is above zero, or within a given interval, or by modeling sub-threshold synaptic currents.
% The former approach has been studied in works such as \cite{}
% Should talk about points relating to where rate-based metrics is the best one can do wrt spikes
The same often holds for SNNs in which surrogates are constructed, due to elements such as the initial model configuration, initial conditions, or model perturbation.
Works such as \cite{Jin2018} employ a rate-based error metric for data and readouts pertaining to image classification tasks.
By considering how these complex systems behave differently under different conditions, and that they may be heavily affected by their current and thus past state, approaches pertaining to rate codes has seemed to be the extent to which it is possible to incorporate temporal precision whilst successfully optimising and inferring models.
While these works are valuable in studying how one might combine optimisation and spiking models, they often are limited not only to rate-based encodings and/or loss metrics, but also to non-noisy ML domain types of data.
However, the sole basis for studying SNNs in this work is to maintain the biological parallel, such that the model may be used as an interpretable system and hypothesis of recorded site dynamics, and studied as such.
Therefore, it is important to study how the precise timing of spiking may be made relevant in the models, as that is arguably not only a relevant source of information, but a rich encoding scheme employed by the neural circuitry in the biological brain.
While this was initially debated, it has been argued both by information theorists, neuroscientists and computer scientists alike recently that the precise timing of spiking is not only relevant, but is in fact necessary in order to encode all of the information that has shown to be processed by the brain, i.e. a rate-coding is not enough to represent a task which is solved by a neural network.

In my work I have studied employing gradient based optimisation as well as approximate Bayesian computation using amortised learning for both a probabilistic class of stochastic spiking neural networks \cite{Rene2020}, for leaky types of integrate-and-fire SNNs \cite{allen_glif_white_paper}, and for closed-form (still with multiple solutions?) exact-gradient types of non-leaky integrate-and-fire SNNs \cite{Huh2017}.
I have also replicated and analysed Izhikevich SNNs, but chosen not to apply GBO to this model class due to planes of chaos in the parameter landscape, simply resulting in that the Izhikevich neuron ODEs are a lower-dimensional projection of the Hodgkin-Huxley ODEs - resulting in that while they are of lower complexity simulation-wise, the collapse of several variables into fewer results in variable intervals for which model behaviour is unrealistic and/or chaotic, making GBO impossible.


\section{Bayesian approaches}

We assume that the reader is familiar with Bayesian statistics. However, just to give an outline of our notation to the extent that we may also outline the SBI approach used, we may define a sample of observations $O$ given priors $p$, making the posterior over the parameters given an observation,

\begin{equation}
    P(\theta|X_o) = \frac{P(x_o|\theta)P(x_o)}{P(\theta)}
\end{equation}

One way of going about this is to generate observations with a simulator, for which we may generate samples over priors, enabling us to estimate a full posterior over the parameters for a given observation.
Note that this does not omit the difficulty of model conditions, including model perturbation and external input.
Further, the computational cost requires sampling a great number of samples in order to estimate the posterior over the parameters, thus increasing drastically with the model dimensionality and number of parameters.
The approach does, however, come with the benefit of being able to assess and directly measure the certainty by usual statistical metrics around the centres of the posterior distributions, as well as that the full posterior distribution contains samples across all parameters, dependently, which may capture dependencies not so easily captured when estimating using GBO.

% \subsection{MCMC sampling}
% \cite{Rene2020} "special case" i.e. mesoscopic-microscopic inference using first ABC (or GD?) on pop. level and then MCMC sampling for full approx. posterior?

% “To understand how rich dynamics emerge in neural populations, we require models which exhibit a wide range of dynamics while remaining interpretable in terms of connectivity and single-neuron dynamics. However, it has been challenging to fit such mechanistic spiking networks at the single neuron scale to empirical population data. To close this gap, we propose to fit such data at a meso scale, using a mechanistic but low-dimensional and hence statistically tractable model.”
% Argue that instead of making the reduction to a population-level model, we may instead make the dimensionality reduction for the parameters.
% There exists work on fitting GLIF parameters to physiological data. 
% Also, inferring weights with fixed parameters is possible using GD, given data.
% This could mean that we could, given the correct neuron classes, infer a topology or the connectivity of a site recorded from!
% Parameters may be inferred prior to the weights using an appropriate procedure.
% However, in the Rene-paper they use aggregate population activity, or spike histories, and derive the likelihood for the parameters. Can be used to either optimise, or do Bayesian inference.
% They conclude that population inference impact and improve the accuracy in single-neuron parameter inference.

% The René paper assumed known input when inferring models. They also generated data with sine-modulated input.
% They also made a quasi-renewal approximation, and assumed independence between neurons in each time bin.


\subsection{Amortised learning with DNNs}

% SNPE
\cite{Lueckmann2018} propose using sequential neural posterior estimation, in which a simulator network to generate samples, and a deep neural network to approximate the posterior over the parameters given the sampled observations.
This can then be used to estimate the posterior given a target observation, which may be outlined as,

\begin{math}
    O: \Phi(\theta) \sim X, \quad
    DNN = F_\Phi(\theta; X) \approx P(\theta|X)
\end{math}

% Since DNNs are universal function approximators, they may be used to approximate the function of the posterior distribution over the parameters given observations for a given model system.
It seems that DL is an excellent candidate for this type of function approximation, and that in this regard GBO works just as well as it does in ML, depending on how well-defined the posterior distribution in fact is in the data.
We find in our work that particularly when working with models that have larger spaces which I like to think of as "frontiers" in the parameter landscape for which the loss is approximately equal, the posterior is similarly not condensed around fixed points.
\cite{Lueckmann2021}, however find that the posterior approximation still contains the relationship, i.e. dependence, over the parameters. % TODO: Check ref.
In other words, while they cannot be sampled independently for marginals, sampling over the full posterior still generates realistic data, even if necessarily still suffering from the same local minima samples from the posterior, due to the input-output ambiguity.


\section{Evaluating SNN models}

% Not straightforward how to analyse... 
% Should this be in background or later?
% Refer reader to report or poster in which these are explained, and only provide "glue"-text here, explaining usage and placing them in the context?

While it is non-trivial to exactly evaluate the information processing of two networks, we may use statistical measures such as spike correlations, or dimensionality reduction methods to identify prominent features, or modules of co-active neurons.
As mentioned previously in the introduction, we use non-negative matrix factorisation (NMF) \cite{Seung1999, Seung2001} in order to identify these sets of co-active neurons in a spike history, which have been found to be good low-dimensional abstractions of functional behaviour, further strengthened by the finding that factorised modules are as good state predictors as the raw neuron-signals when applying linear disctiminant analysis (LDA) or random-forest regression, as done in \cite{Onken2016a}, and replicated in \ref{chpt:sleep}.

\subsection{NMF}

% In order to assess functional organization in the neural data we employ non-negative matrix factorization (NMF) \cite{Seung1999} to infer ensembles of coactive neurons. 
% NMF is a dimensionality reduction technique which has been shown to be particularly suitable and well-performing for neural data \cite{Onken2016a}.
The NMF factorization may be written as,

\begin{equation}
    M \approx WH,
\end{equation}

where the original data set $V$ of dimension $n \times t$ is factorised into two matrices of dimensionality $m \times n$ and $m \times t$.
What makes this dimensionality reduction method particularly suited for spike train data is its non-negativity constraint, which naturally leads to a parts-based representation. % Further, NMF has been shown to outperform other common dimensionality reductions such as independent component analysis, principal component analysis, and factor analysis \cite{Onken2016a}.

While NMF may be used to discover spatial and temporal firing patterns in spike data, the geodesic similarity measure between the modules may be used to assess how well these features were recovered in a fitted model.
The geodesic distance $o$ between two matrices $A$ and $B$ of dimensionality $m \times n$ may be written as,

\begin{equation}
    o(A, B) = (1-\frac{2}{\pi}\cos^{-1}(A^TB)),
\end{equation}

where $m$ is the number of modules, and $n$ is the number of nodes in the data set. 

% \subsection{LDA}

% % linreg for separating hyperplane with arbitrary kernel? or simply linear transformations?
% % \subsection{Random forest}
% \cite{Onken2016a}


\subsection{Generalised Linear Models}

In order to assess the extent to which the spike statistics are encompassed by inferred models, a suitable baseline model is the generalised linear model (GLM) \cite{Nelder1972, Fernandez2000}, as this model is well-suited for capturing the statistics of spike trains.

For binned spike trains, the spike counts may be assumed to be Poisson distributed, and may thus be approximated by a Poisson PDF.
Correspondingly, we may model each neuron as a Poisson PDF, with the rate parameter $\lambda$ giving a probability for spiking, or an approximate number of spikes for an interval.
In the GLM model, this is extended by linking the observed spike counts $\mu$ with the probability distribution's (here Poisson) corresponding density function - or rather the inverse of it, assuming that the observed variable is a linear combination of a set of parameters, which in turn lets us numerically fit the GLMs to a spike history.
Further, we may also choose to couple each node's spike response with the other's activities, thus also assuming dependence on these. This may be written as,

\begin{equation}
    y_t|x_t, w \sim Poiss(y_t; f(X_t^T w)\Delta),
\end{equation}

i.e. the predictor (via the Poisson PDF link function) given the observed activity and a set of weighted couplings between the other nodes gives models that may capture the spatiotemporal statistics that may be inferred by maximum likelihood estimation with linked Poisson PDFs given the observed spike histories, weighted also on the other nodes' estimations.
This gives the log-likelihood of

\begin{equation}
    log p(y|X, w) = \sum_{t=1}^T log p(y_t| x_t, w) = \sum_{t=1}^T(y_t log f(x_t^Tw) - f(x_t^Tw)\Delta)
\end{equation}

which may be approximated using MAP estimates by numerical methods.

% Coupled GLMs, in which spike histories depend on one another, may also be approximated. This allows to capture second-order and potentially higher-order spike correlations, depending on the link functions used.

% GLMs are a suitable basline model for spike train data, as spike trains are Poissonian in nature, and thus fitting parameters for a Poisson probability distribution for each node provides spike trains of similar nature.
% Further, to capture correlations between neurons in spike trains, this definition may be expanded over a set of probability distributions, where each is parameter is dependent on the others.

% For more details, please see the references included above, or the sample code given in \ref{appendix:sample_GLM_code}.


\section{Model definitions}

\begin{figure}
    \centering
    \vskip -0.1in
    \includegraphics[width=0.49\columnwidth]{figures/samples/membrane_potentials/export_sample_LIF_white_noise.eps}
    \includegraphics[width=0.49\columnwidth]{figures/samples/membrane_potentials/export_sample_GLIF_white_noise.eps}
    \includegraphics[width=0.49\columnwidth]{figures/samples/membrane_potentials/export_sample_mesoGIF_white_noise.eps}
    \vskip -0.1in
    \caption{membrane potential samples}
    \label{fig:membrane_potential_samples}
    \vskip -0.2in
\end{figure}

\subsection{LIF}

% Write out definition, discuss a bit..? Refer to paper.
% Included portion of paper:

The neuron model we employed in this work is the leaky integrate-and-fire (LIF) model, as described in among other works \cite{Rolls1998Book}, and also in 
\ref{chpt:LIF}. For the sake of consistency, we include a brief description here in this chapter. The LIF model may be formally outlined as,

\begin{equation}
    \frac{dv}{dt} = \frac{E_L - v_t + R_I I_t}{\tau_m},
\end{equation}

where $E_L$ is the rest potential, $v_t$ is the membrane potential at time $t$, $R_I$ is the membrane resistance, and $I_t$ is the synaptic current.
The modelled networks are non-transitively fully connected, with the weights normally distributed between $w \in [-1, 1]$. The synapses are modelled as exponentially decaying post-synaptic currents, where a conductance variable $g$ models the conductance for the neurons,

\begin{equation}
    \frac{dg}{dt} = -\frac{g}{\tau_g},
\end{equation}

where the synaptic input current to a neuron $j$ is modelled as $I_{syn,j} = \sum_{i} w_{i,j} I_{i,j}$.

While forward passes are done according to a spike-threshold function with non-linear resetting of the membrane potential upon spiking, the backward pass is made possible by separately defining a differentiable soft-threshold function over the membrane potential which is used solely for back-propagating the error gradients obtained by minimizing the loss functions. We use the commonly applied sigmoidal function for this purpose,

\begin{equation}
    s_t(v) = \frac{1}{1+e^{-(v_t))}},
\end{equation}

where $s_t$ denotes whether a neuron spikes at time $t$ with a membrane potential $v_t$.
This results in that optimization of defined loss metrics always back-propagates error gradients through the soft-thresholded membrane potential, in effect treating this as a continuous spike value. 
In other words, the distance metrics calculate the distance between a continuous spike train to a binary target spike train.
Note that this also results in that sub-threshold values result in a spike signal during optimization.

% TODO: Update.
% \begin{figure}
%     \centering
%     \vskip -0.1in
%     \includegraphics[width=0.9\columnwidth]{figures/plot_spike_thresh_lif_ensembles_dales_0.eps}
%     \vskip -0.1in
%     \caption{The soft-thresholded spike signal $s_t(v)$ for $60 \si{\ms}$ backward passes during model simulation for $12$ neurons.}
%     \label{fig:spike_thresh}
%     \vskip -0.2in
% \end{figure}

All parameters were held as free parameters during optimization, resulting in 4 $N$-dimensional free parameters, and one $N^2$-dimensional parameter, where $N$ is the number of neurons in the network. 


\subsection{Generalised leaky integrate-and-fire (GLIF)}

% Write out equations (?)
My GLIF implementation is based on the whitepaper of \cite{allen_glif_white_paper}, and may be defined as

\begin{equation}
    \frac{dv}{dt} = \frac{g (E_L - v) + R_I I_{syn}}{C_m},
\end{equation}

where $v$ is the membrane potential, $g$ the conductance, $E_L$ the reversal potential, $R_I$ membrane resistance, $I_syn$ the total incoming synaptic current, and $C_m$ the membrane capacitance.
While the neuron model differential equations are linear, spiking is highly non-linear, and determined by the composite spike threshold $\theta_v + \theta_s$, whose differential equations are

\begin{equation}
    \frac{d\theta_v}{dt} = a_v (v - E_L) - b_v (\theta_v - \theta_{inf})
\end{equation}

\begin{equation}
    \frac{d\theta_s}{dt} = - b_s \theta_s
\end{equation}

where $\theta_v$ is a membrane potential dependent spike threshold, and $\theta_s$ is an exponentially decaying threshold, $a_v$ is an adaptation factor for $\theta_v$, $b_v$ is a voltage-induced threshold time constant, and $b_s$ is a spike-induced threshold time constant.
Introducing these thresholds makes spiking highly adaptive, which may mimic the temporal dynamics of the sodium-potassium pump, whilst remaining linear and differentiable.
Upon $v \geq \theta_v + \theta_s$ spiking occurs, which results in non-linear variable resetting, defined as,

\begin{equation}
    v_{\text{reset}} = E_L + f_v (v - E_L) - \Delta_V
\end{equation}

\begin{equation}
    \theta_{s,\text{reset}} = (1 - b_s) \theta_s + \delta_{\theta_s}
\end{equation}

where $f_v$ is the pre-spike voltage fraction influence on the reset potential, and $\Delta_V$ is voltage addition following reset.

Each neuron is connected to every other neuron in the network, the weights being normally distributed with values $w \in [-1, 1]$. The synaptic currents decay with a factor $f_I$ with an additive after-spike current $I_A$. This may be written as,

\begin{equation}
    I_{syn} = \begin{cases}
        (1 - f_I) I_{syn} + I_A \quad \text{for } v \geq \theta_v + \theta_s,\\
        (1 - f_I) I_{syn} \quad \text{otherwise}
    \end{cases}
\end{equation}

where the synaptic input current to a neuron $j$ is modelled as $I_{syn,j} = \sum_{i} w_{i,j} I_{i,j}$.

GLIF neurons certainly allow for more biological realism in that they may exert most types of behaviour as outlined in the table in figure 2. in \cite{Izhikevich2006}.
As such, their information processing is theoretically greater, and the types of spike patterns they may exert are richer than when compared to LIF SNNs.


\subsection{Non-leaky integrate-and-fire (NLIF)}

% Refer to \cite{Huh2017}, mention impact, refer to synapse chapter.
Interestingly, using sub-threshold synaptic currents as the spike signal, rather than a surrogate function over the membrane potential, results in not only a continuous optimisation signal, but also one that has a specific temporal signature, depending on neuronal excitation.
Extending the work of \cite{Huh2017}, we implement first their non-leaky integrate-and-fire (NLIF) neuron model, with the aforementioned sub-threshold synapse model, which is defined using a gating-function which essentially results in a synaptic current when the potential is inside of an active zone.

\begin{equation}
    \int s dt = \int g dv = 1,
\end{equation}

where $s$ is the synaptic currents, and $g$ is the current gating function, and the synapse model is,

\begin{equation}
    \tau_s \frac{ds}{dt} = -s + g \frac{dv}{dt}
\end{equation}

Combined with a lower-dimensional target signal this enables perfectly learning to perform the task, albeit with the inferred model parameters depending largely on the initial configuration and random seed.
We extend their work to also test their methodology on LIF SNNs (see chapter \ref{chpt:frontier}, and find that although this results in non-exact gradient calculation, the setup is sufficient for inferring models with near-perfect performance in reproducing the target signal for LIF SNNs, too.

\subsection{Stochastic integrate-and-fire models (SGIF)}\label{microGIF}

Lastly, to tie my work with a current state-of-the-art procedure, I have implemented the cortical microcolumn-inspired (based on \cite{Schwalger2017}, adapted to a lower set of neurons) stochastic integrate-and-fire (SGIF) population model of \cite{Rene2020}, and extended it such that it is compatible with in-place gradient based optimisation. 
We compare the results on both population- and neuron-level model fits with previously reported results, and also propose that the GBO procedure may be used to directly infer heterogeneous neuron-level models, and test this by fitting models to a full-size SNN SGIF target model.
We also test fitting SGIF models to biological data, and evaluate the goodness of fit using loss metrics and geodesic similarities between the NMF modules.

While we refer the reader to \cite{Rene2020} for the full model definition, we wish to outline the crucial parts to making the models in-place differentiable - not requiring using approximate Bayesian computation over model samples in order to estimate a posterior.
The synapse model may be defined as,

\begin{equation}
    \tau_s I_{syn}(t) = W_{syn} \epsilon_s(t),
\end{equation}

\begin{equation}
    \tau_s \epsilon_s(t) = (1 + \tanh(t_{s} - \Delta_s)) e^{-\frac{t_{s} - \Delta_s}{\tau_s}},
\end{equation}

where $t_s$ is the time since the previous spike, $\Delta_s$ is the delay before synaptic transmission after spiking, $W_{syn}$ are the synaptic weights, and $\epsilon_s$ is the synaptic kernel, or spike-transmission model.
Note that the key difference between this formulation and the one in \cite{Rene2020} is using the $\tanh$-function to incorporate the transmission delay, rather than by using a Heaviside function $\Theta$, as this allows us to differentiate the function, and thus backpropagate error gradients to calculate parameter gradients.
Thus, we may use negative log likelihood estimation over the spike probabilities, i.e. maximising the likelihood that a set of spike probabilities produce the target spike trains when drawing from a Bernoulli distribution with the probabilities, which is equivalent to minimising the negative log likelihood of the target spike train given simulated probabilities.

% Based on the work of \cite{Rene2020}, which is based on \cite{Schwalger2017}.
% Spike probability, rather than membrane potential and spikes.
% --> Minimise negative log-likelihood under a Bernoulli or Poisson assumption.

% ALlows for a direct comparison.


% % TODO: check: "frontier" found using frd - vrd obscures gradient and diverges more.
% BNLL \& PNLL "frontier"

% \subsection{GLIF neurons}
% Can be highly parameter-sensitive and result in completely different mode of neuronal behaviour


\subsection{Izhikevich}

In our initial SNN research, we replicated \cite{Oliveira2019} to show that the resulting rates could be formulated more or less as a linear function, resulting from sub-threshold oscillations, and that the model is highly sensitive to its initial parameter values.
One of the motivations for doing this was that the model class may exert all neuronal modes of behaviour as observed in biology \cite{Izhikevich2006}.
However, as hinted to above, this results in a highly irregular parameter landscape, only further complicated by the previously discussed stochasticity present in the model inference scheme that we are studying.
To give an intuition about why this is, consider that the Izhikevich model is a 2-dimensional projection of a 4-dimensional ODE system - in fact based upon the original HH model \cite{HH1952}.
As such, there are regions for which unstable, or even chaotic behaviour, may emerge - i.e. we end up with parameter regions for which the model is highly unrealistic, or not well-defined.
Therefore, we limit our research to the previously outlined models, and only include parameter landscape plots illustrating the aforementioned, as well as our work on the effect of the recovery variables on resulting subthreshold oscillatory behaviour.

% TODO: Include param landscape figures here??

% All modes of behaviour as observed in biology, however something needed to shift mode(s) of behaviour..
% Parallels to SNN inference etc
% (2D ODE syst per neuron)

\subsubsection{Poster: \textit{The effect of the recovery variable parameters on oscillating Izhikevich networks}}\label{section:izhikevich}
\includepdf[pages=-]{files/berg_and_onken_uk_neural_computation_2019_poster.pdf}


\section{Loss metrics}

% simple, rate-based metric. does not capture correlations, or only very vaguely if done over smaller time bins. 
% can be combined with correlation metric.

% Tricky design, stochasticity, lots of considerations, as we shall see only frontier..
When it comes to designing suitable loss metrics, this is one of the main challenges for applying gradient based optimisation to SNNs, as it is what defines the gradients, and thus the parameter space that we traverse during optimisation.


We performed optimization using (1)~the firing rate distance, (2)~the van Rossum distance, (3)~an additive combination of the van Rossum and firing rate distance, (4)~a Pearson correlation metric distance, (5)~the Fano Factor, (6) the mean squared error, and (7) the negative log-likelihood assuming either a Bernoulli or Poisson PDF for the probabilistic models.
% emphasising mainly the firing rate in earlier training iterations, and conversely mainly the van Rossum distance in later training iterations. 

\subsection{Firing rate distance}

We define the firing rate distance as the Euclidean distance between each neuron's average firing rate for a given spike interval, which may be written as,

\begin{equation}
    d_r(S_1, S_2) = \sum_i^N{\frac{\sqrt{(s_1^i - s_2^i)^2}}{\Delta t}},
\end{equation}

where $s^i$ is the number of spikes for neuron $i$ in a given interval $\Delta t$.

\subsection{van Rossum distance}

\begin{figure}
    \centering
    \vskip -0.1in
    \includegraphics[width=0.9\columnwidth]{figures/samples/neur_vr_conv_sample.eps}
    \vskip -0.1in
    \caption{placeholder: van rossum convolution example}
    \label{fig:vrd_conv_sample}
    \vskip -0.2in
\end{figure}

One thing is quantifying distances in terms of spike metrics.
Another is designing a metric that is well-defined for gradient-descent.
While the van Rossum metric describes the distance between two spike trains both in terms of rates and timing, due to its sensitivity to exact timing the gradient signal might be obscured for non-matching neurons, and more well-defined where the rates better match.

% \subsubsection{van Rossum distance}
Hypothesis: Using the van rossum distance may guide search, allowing gradient-based methods to converge?
Using the van-rossum distance as a loss function for the spike trains, we get something more continuous, since the discrete spike train is convolved with a time-based kernel. 
This should make the gradients more informed via the loss function in larger search areas.

% figures of a convolved spike train, reference to LIF poster

% for probabilistic SNNs, as well as GLIF(?) SNNs:
Main finding: 
"\textit{wandering gradients}" for vRD.
precise timing emphasis. breaks down for smoothed and stochastic signals, only works well for NLIF, or when it approaches rate-based - even then FRD might outperform this metric for surrogate GD.

The van Rossum distance \cite{VanRossum2001} may be defined as the Euclidean distance between two spike trains where each spike is convolved with an exponential kernel forward in time,

\begin{equation}
    f_{conv}(t, t_{i, spike}) = e^{\frac{-\Delta t_i}{\tau_{\mathrm{vr}}}}
\end{equation}

where $\Delta t_i = (t-t_{i, spike}),\ t \geq t_{i,spike}$, 
with $t$ being the current time, $t_{i,spike}$ the most recent time of spiking for neuron $i$, $\tau_{\mathrm{vr}}$ a time constant (set to $\tau_{\mathrm{vr}} \in \{10.0, 20.0, 100.0\} \si{\ms}$ in the experiments, and $\Delta t_i$ the time since the neuron's last spike, $\Delta t_i = (t-t_{i, spike})$, the distance between two spike trains is then given by the Euclidean distance between two convolved spike trains,

% potential figure of convolved spike train?

\begin{equation}
    d_v = E(S_1, S_2) = \sum_{t=0}^{t=N} \sqrt{(f_{conv}(S_1)-f_{conv}(S_2))^2}
\end{equation}


% \subsection{KL-divergence}

% "different prob dists"

\subsection{Maximum Likelihood Estimation}

% Negative log-likelihood (Bernoulli, and Poisson PDF assumptions) for probability-based models.
By assuming either a Bernoulli or Poisson distribution for the observations, or spike trains, we can maximise the likelihood of producing that same data by using numerical methods.
As usual, it is easier to work with a sum of products, rather than exponents, when doing numerical differentiation over a probability distribution to find the maximum likelihood estimate (MLE).
Therefore we maximise the log-likelihood over the distribution, as it lets us work with a sum of products instead of a product of exponential functions, and is equivalent to maximising the likelihood itself.
In fact, we also instead minimise the negative log-likelihood, which remains equivalent to maximising the likelihood, as this lends itself directly to optimisation as a loss metric and error signal which may be used for gradient-calculation.

\begin{equation}
    \mathcal{L}(\theta | x) :\propto - \log \mathcal{L}(\theta | x) = - \log \prod_{n=1}^N pdf(\theta; X) = - \sum_{n=1}^N \log (pdf(\theta; X))
\end{equation}

Assuming a Bernoulli distribution, $pdf=Bernoulli(\theta; x)$, or assuming a Poisson distribution, $Poiss(\theta; x)$, these let us estimate the maximum likelihood over $\theta$.


% =======================================================
\chapter{Gradient-descent based LIF SNN inference}\label{chpt:LIF}
\section{Poster: \textit{Spiking Network Inference Using Gradient Based Optimisation}}
\includepdf[pages=-]{files/FENS-virtual-2020-e-Poster-W-P-Berg-and-A-Onken-Spiking-Network-Inference-Using-Gradient-Based-Optimisation-PORTRAIT-5.pdf}

\section{Report/paper draft: \textit{Parallel spiking neural network parameter inference using gradient based optimization}}
\includepdf[pages=-]{files/icml2021_draft_snn_inference.pdf}


% =======================================================
\chapter{The frontier of SNN inference}\label{chpt:frontier}

In order to study how to infer spiking neural network models (SNNs) we need several ingredients: (1) a definition of the system, (2) an algorithm for model inference, and (3) an implementation of (1) and (2), where the algorithm for the case of gradient descent also necessarily contains (3) a way to measure and assess model performance, which is ideally comparable with existing methodologies and models.

Research on SNNs using gradient-based optimisation is fairly limited, and includes approximate Bayesian computation for smaller models, conversion learning in which more traditional ANNs are transformed to simpler SNNs after training, or surrogate gradient descent, in which a surrogate signal over the model spikes, usually as a function over the membrane potential, is used in order to obtain a differentiable output that may be used to optimise the model's parameters over a given loss metric by backpropagating the error signal. Notably, there exists some research on gradient descent for SNNs in which the model is defined such that its parameters may be used directly for error signal gradient backpropagation, including \cite{Huh2017}, \cite{Rene2020}. This is slightly different from surrogate gradient descent in that the backpropagation is a more natural extension of the system, and thus albeit remaining unrealistic in terms of biology, it is closer to learning in a Hebbian and spike-time dependent plasticity (STDP) fashion, as noted by the authors in \cite{Huh2017}.

% Currently, the state-of-the-art methods are only partly successful in training a subset of SNN model parameters for smaller models.
With the goal of accelerating SNN inference research, I looked at the most prominent state-of-the-art methodologies for SNN inference, which may be divided into (1) gradient based optimisation (GBO), and (2) approximate Bayesian computation (ABC) \cite{Lueckmann2018, Rene2020, Cranmer2020a, Lueckmann2021}.
GBO enables leveraging recent ML advances from deep learning, and albeit biologically implausible as a learning rule in its own right, allows for in-place inference of models that are biologically realistic. The computational cost of doing gradient based computation is also exponentially less than that of doing ABC, as this involves doing a costly (Monte-Carlo) sampling step \cite{Rene2020}.
Therefore, GBO is the main focus of this work, and leveraging system definitions that may allow for more information to be captured and propagated by means of gradient descent types of optimisation is therefore also something that is desirable, and notably consistent with the works that have been extended.

As for ABC, Bayesian inference has until recently been intractable for SNNs due to their high-dimensionality. However, recent advances in deep learning, leading to the advent of sequential neural posterior estimation (SNPE) \cite{Greenberg2019a, Durkan2018, Goncalves2019, Cranmer2020a}, in which a DNN is trained to estimate a posterior over a prior given observations, has allowed for much more efficient sampling using this amortised approach; sampling from the DNN to perform posterior parameter estimation (which in turn uses GBO in the posterior inference, i.e. for fitting the DNN).
Note however that this procedure still has a high cost due to the sampling procedure, and does not scale well with an increase in the network size.
% references, brief summary and discussion of main references
Using a Bayesian approach, we may however estimate the posterior over the model parameters, given our approximation of the prior. This also yields a type of uncertainty estimate around the posteriors. Note however that the posterior will be skewed by the prior approximation, in our case given by SNPE.
 
Using a surrogate gradient approach, we may fit the parameters of the system by error minimisation using a differentiable loss function. 
By working with biological spike train data, which is a very widely used format, we cannot however make many assumptions about the input, unless the recording somehow also contains information about this signal. 
Therefore, we have to model the input in some way, taking care in considering how assumptions may affect model behaviour during learning. 
As a rule of thumb, we want to have a signal which allows for the network to perform the task at hand, or to replicate the observed data, whilst remaining biologically reasonable.
In other words, we want to have input that puts the system in a realistic mode of behaviour, and yet allows us to optimise its parameters.
This means providing an informative signal, which is meaningfully transformed by the model with regards to output error minimisation, and thereby also usable in this regard.
% As it is limited what we may assume about the input, loss metrics that put a high emphasis on the precise timing of spiking may in fact obscure the gradient signal by incorporating too much of the noise stemming from this signal.
% We therefore focus on metrics that are more rate-based, looking at intervals of spikes in order to include the temporal signatures to some extent.
% Using a timing-oriented metric over larger time bins may even result in divergence, due to variability between the hidden target data input, and the inferred model input. 


% \section{My contribution: Implementing, differentiating and optimising different SNN models using different loss metrics}
\section{GBO compatible SNN framework in PyTorch}

% TODO
% Goals and questions or hypotheses

In order to test the extent to which gradient based optimisation may be leveraged for spiking neural network model inference, I have developed and implemented a novel framework in PyTorch.
It allows performing gradient descent over a specified loss metric, for any differentiable SNN model definition, in a highly modular way.
I have implemented a surrogate gradient approach over the membrane potential itself for LIF and GLIF neurons with rate-based metrics as well as spike-time dependent metrics, and over the spike probabilities by using the negative log-likelihood for SGIF neurons, as well as for non-leaky integrate-and-fire neurons by directly using the synaptic currents as readout signals, and considering a more high-dimensional output target signal.
Notably, I extended the latter synaptic current model, as originally presented by \cite{Huh2017}, and combined it with leaky integrate-and-fire models, for which optimisation was highly successful. The work with the synaptic current modification is presented later in chapter \ref{chpt:gated_synaptic}.
% As spikes are binary in nature, by implementing surrogate subthreshold spike signals, we may facilitate optimisation, improving optimisation performance for gradient descent.

As for hypothesising the extent to which GBO may be used to retrieve model parameters that are close to the ground-truth values, we may assess this by fitting to synthetic data.
We hypothesise that we will retrieve local minima which is not close to the ground-truth values where this is accessible, but that the models will however capture the spike statistics to a large extent, potentially allowing for functional analysis and model probing for further hypothesising. 
This can be understood by considering the error landscapes formed by the loss metrics, which we shall plot later in this chapter in order to illuminate this.
Further, we shall also test the extent to which we have captured similar ensembles in the spike trains of the fitted models, by comparing the modules attained by non-negative matrix factorisation, a dimensionality reduction technique well suited for neural spike train analysis due to its non-negativity constraint, irrespective of the parameter values of the fitted models.

% As such, it may be used to evaluate the functional characteristics of the modeled nuclei.
% The input is unknown.
% SBI approach for SNN inference; limitations. Test potential.
% Better for true parameter estimation. However,  posteriors are probability densities, and do not have direct access to full model configurations as such, and may suffer from not capturing full specific model configurations that capture as many higher-order spike statistics.
As a baseline model for comparison in NMF module similarity, we use generalised linear models (GLMs), which are good candidates for capturing spike statistics in the NMF analysis due to their exponential family (and here Poissonian) formulation.
However, we hypothesise that GBO for SNNs may enable capturing a higher factorised module similarity than the GLMs fitted by maximum likelihood estimation (MLE).
If so, this could either be due to an inductive bias (by model definition), or due to the richer range of behaviours available to the SNN models.
% Input transformation (?, avg. relative spiking per population ?)
% net(x\_forward)
% Output transformation (avg. v per pop.)
% Can transform spike-train to average population membrane potential
% Synapses with dg/dt ?
% self.g = torch.ones\_like(self.v)  for one conductance per neuron (for n synapses)

% \subsection{Contribution: PyTorch}

% ... tricks, sequentiality etc
Current implementations of SNNs in the field are commonly done in Brian 2 or Matlab. 
Upon commencing work on SNNs using LIF neurons, the only recent work I found that supported optimisation in a supervised or semi-supervised manner was in Theano, a legacy Python library for symbolic programming.
With recent advances in ML not only through increased computational resources and novel optimisation techniques, but also in programming libraries that also compile to and run on the GPU, and in either case contains highly optimised library code implemented in a lower-level programming language (typically C, C++, or Cuda), we wanted to make use of this aspect too, and after too many hours of fighting with at best poorly documented legacy Theano-library code, we decided to set out to implement SNNs in PyTorch.
This led to the implementation of a modular framework and pipeline of SNN inference using GBO in PyTorch.
Key to allowing using PyTorch, is maintaining a neuronal state throughout simulation, which I implemented by enforcing sequential input propagation throughout the model, such that the internal state is updated for each time-step.
With this simple trick, intervals of input mutate the state with each input corresponding to some desired time-step constant, in my code set to $1 \si{ms}$ for simplicity.
However, it does result in that intervals of simulation need to be performed for each backward pass, i.e. updating the model parameters given error gradients.

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/samples/SameModelClassTarget/export_spike_trains_euid_12-10_09-45-02-423.eps}
    \includegraphics[width=0.49\columnwidth]{figures/samples/SameModelClassTarget/export_GLIF_plot_loss_euid_12-09_16-17-13-464.eps}
    \caption{Target and fitted GLIF model spike trains, and loss per training epoch, membrane potentials in figure \ref{fig:sample_GLIF_vs}}
    \label{fig:sample_GLIF_spikes_loss}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/samples/membrane_potentials/export_sample_GLIF_white_noise.eps}
    \caption{membrane potentials for the fitted GLIF model in figure \ref{fig:sample_GLIF_spikes_loss}}
    \label{fig:sample_GLIF_vs}
\end{figure}

We implemented a general SNN optimisation framework on top of PyTorch \cite{Paszke2017} and utilised PyTorch's autograd-feature for backpropagation of error gradients.
Key to enabling backpropagation is defining a differentiable output signal. Traditionally, the sigmoid function is a common choice, i.e.

\begin{equation}
    s_t(v) = \frac{1}{1+e^{-(v_t-(\theta_v + \theta_s)}}
\end{equation}

where $s_t$ denotes whether a neuron spikes at time $t$ with a membrane potential $v_t$. Or by considering the spiking to be a readout of a continuous parameter/variable such as the post-synaptic current from each neuron, as is done in the case of the continuous sub-threshold synaptic model in \cite{Huh2017}, or optionally the probability of spiking, in a probabilistic formulation of integrate-and-fire SNNs, as in \cite{Rene2020}.

% ref. Adam
% synthetic data generation with pseudo-random Poisson input, data driven experiments in vivo data set.
To perform optimisation, an initial model parametrisation is drawn uniformly from parameter intervals that are constrained to meaningful parameter values, and then perturbed with input drawn from some input generator function. Note that the parameters of this function may also be fitted.
The model output is then compared with the output of a target data set, and the specified loss metric and distance is calculated between the produced model output and target spike train.

\subsection{The framework}

% Modular Python-based framework written in PyTorch that allows for automatic differentiation of the computational graphs defined with PyTorch-code.
We implemented a framework in Python/PyTorch, performing differentiation \& optimisation using the autograd-feature of PyTorch.
Further, PyTorch allows for compactly defining the models by using the library code, which again is compiled into a computational graph, which in turn is able to be compiled into and run using the pre-compiled C++ or Cuda library-code.
Note that as such, this does not only allow for automatic differentiation using the computational graph constructed by defining nodes, edges, and leafs in the graph, but model simulation itself becomes fast when only running library-code, and not escaping to the Python interpreter.
While a comparison of run-time speed is out of scope in this thesis, it is often possible to reach a level close to that of C/C++, or Cuda - but with a main bottleneck in this case being imposed by the temporal limitation of SNNs; namely that simulation must be done whilst maintaining the model state in time throughout each simulation interval.

For the framework itself, the code may be found openly available at \href{https://github.com/williampeer/snn_inference}{github}, I structured this into parts roughly divided into packages for models, analysis, utilities, experiments, and tests, with test coverage of most of the code.

One of the main entry points allows for selecting an experiment type, model type, loss metric, and optimiser, with plugging in an external data set being optional - if not a random target model of the same class as the set model type is used to generate synthetic data.


\subsection{Batching and SNNs}

Despite the need to do simulation in a sequential manner as described above, I implemented iteration over batches of activity, effectively allowing for a type of batch normalisation, and increasing the parallelisability of the algorithm with the number of batches run in parallel, allowing for better use of the computational resources at hand.


\section{Experiments}

Each experiment consists of fitting a model to a data set synthetically generated by a hand-engineered model of the same class.
The target models and data were constructed such that they produced an array of behaviours and spike correlations, and which most importantly do not lie in a chaotic behavioural regime.
This allows for comparing inference performance by both the loss metric and the average parameter distance, since the ground truth values of these are available.
For a comparison across model classes fitted to the same data set, as well as for biological data, we report our work on this in chapter \ref{chpt:sleep}, in which we fit to biological data from in vivo recordings.

% \subsection{Optimisation}

Returning to GBO for the synthetic data; for each experiment, a model is pseudo-randomly initialised by first setting a random seed (such that results may be reproduced) and then drawing initial parameter values uniformly from pre-defined parameter intervals, which are set such that the model is in a non-silent and non-chaotic mode of behaviour, with the ranges being set to biologically plausible values.
Then, each model is fitted until either the average gradient has decreased to a fraction ($\approx 1 \%$) of the initial average gradient, or for a set number of training iterations, $N_{exp}=100$.
Each training iteration consists of model simulation for an interval of $t=1200 \si{ms}$, and then updating the parameter values according to the gradients.
The target data is synthetic data generated by the same model class, such that we may compare the retrieved parameters with the ground-truth.
Data was generated as previously mentioned by hand-engineered models of the same class (i.e. LIF, GLIF, or SGIF).
Note also that all of these model types are to biological spike train data in chapter \ref{chpt:sleep}.
Also, for the SGIF class, we also calculate the Pearson correlation coefficient, as well as root mean squared error (RMSE) between the produced and target signal, in order to compare model performance with the results reported in \cite{Rene2020}.

Since some model parameters are more sensitive than others, we define linear constraints for each parameter, which PyTorch allows for incorporating into the computational graph in a simple manner by defining a hook for each backward pass over the parameters, i.e. for each gradient update, effectively clamping the gradients such that the parameters do not wander outside of realistic intervals. 
These intervals are naturally far wider than the initialisation intervals, but by considering what intervals are realistic, this is a simple and natural way to exclude unrealistic modes of model behaviour whilst constraining optimisation simultaneously.
Note that the constraints do not pose small intervals of possible values for the parameters, but merely function as limiters enforcing realistic ranges for them, and as a side effect may help avoid divergence by over-adjusting a sub-set of the parameters, typically during the first epochs of optimisation.

% SBI is also different from what René et al. did in their paper. So these are two methods to compare with their findings. 
% \section{Results}

Prior to the main experimental setup above, I simulated each model class for realistic intervals of parameters, whilst fixing the other parameters to values that I had empirically or analytically determined to be able to produce meaningful output, and plotted the loss and rate when compared to the hand-engineered target model of the same model class, essentially performing a grid search over parameters allowing to plot the marginals of the parameter landscape formed by the loss metric and target model signal.
The results which are included below illuminate how optimisation traverses the gradients formed by the parameter landscape, and gives an idea about the extent to which the procedure is applicable to the model class.


\subsection{Error and parameter landscapes}\label{sect:e_landscapes}

When performing gradient descent, we traverse an error landscape given by the loss metric, and iteratively update the parameter values by moving a certain amount (often called the step size) in the direction along the error gradient. 
This updates the parameters to values that would result in a lower loss for the current data interval at hand, for which the loss was computed.
As may be seen from this more conceptual description; in order for gradient descent to work, the error signals need to be informative over the space of potential true values, and continuously defined for paths that lead to the regions that may contain the target parameter sets and values, or minima.

As we shall see below when plotting 2D projections of the loss across different parameter combinations, this isn't the case for all of the parameters and model types, especially when only considering a rate-based metric.
Note however that this is to be in part expected particularly for a rate-based metric, as the loss signal is to some extent oblivious to the timing of spiking in the data, as it only considers the neuronal rates.
This does not render the loss metric unusable, as we may expect to capture the rates well with a rate-based distance metric - however, the ambiguity of the optima is an issue wrt retrieving the true ground-truth values.
The ambiguity of the error landscape formed by the rate based metrics is illustrated in figures \ref{fig:p_landscape_hmap_GLIF}, and \ref{fig:p_landscape_hmap_LIF} included in this section.

When it comes to the likelihood metrics that calculate the likelihood of producing the target spike train given the simulated probabilities, and assuming either a Bernoulli or Poisson distribution, over the model parameters, arguably seems to be somewhat more suited for retrieving sensible parameter-configurations.
However, as illuminated in part by figure \ref{fig:p_landscape_hmap_SGIF}, these metrics are also ambiguous both in terms of retrieving the ground-truth, and defining a constrained set of configurations.

% For these cases, the parameters settle into a point soon after wandering into the region with very low loss, which is then a local minima in the numerical optimisation procedure.

% GLIF
\begin{figure}
    \centering
    \vskip -0.1in
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/GLIF/test_export_2d_heatmap_N_4_loss_tau_s_tau_m.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/GLIF/test_export_2d_heatmap_N_4_loss_b_s_a_v.eps}
    \vskip -0.1in
    \caption{p landscape GLIF $\tau_s, \tau_m$ (left), and $b_s, a_v$ (right)}
    \label{fig:p_landscape_hmap_GLIF}
\end{figure}


\begin{figure}
    \centering
    \vskip -0.1in
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/LIF/test_export_2d_heatmap_N_4_loss_tau_m_tau_s.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/LIF/test_export_2d_heatmap_N_4_loss_E_L_tau_m.eps}
    \vskip -0.1in
    \caption{p landscape LIF $\tau_s, \tau_m$, and $E_L, \tau_m$ (right)}
    \label{fig:p_landscape_hmap_LIF}
\end{figure}


% \subsection{Stochastic integrate-and-fire GBO}

% Bernoulli and Poisson NLL for optimisation.
% Definition outlined in \ref{chpt:background}.

% Population-level GBO for comparison with \cite{Rene2020}.
% Results included below:
% results spike correlations and RMSE, report comparatively too.

% mesoGIF
\begin{figure}
    \centering
    \vskip -0.1in
    \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_4_loss_c_Delta_u.eps}
    \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_4_loss_E_L_tau_m.eps}
    \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_4_loss_tau_m_tau_s.eps}
    \vskip -0.1in
    \caption{p landscape using frd metric for population (N=4) SGIF model}
    \label{fig:p_landscape_hmap_SGIF}
\end{figure}

% \begin{figure}
%     \centering
%     \vskip -0.1in
%     \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_4_loss_tau_theta_J_theta.eps}
%     \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_21_loss_c_Delta_u.eps}
%     \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_21_loss_E_L_tau_m.eps}
%     \vskip -0.1in
%     \caption{p landscape SGIF 2}
% \end{figure}

% \begin{figure}
%     \centering
%     \vskip -0.1in
%     \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_21_loss_tau_m_tau_s.eps}
%     \includegraphics[width=0.32\columnwidth]{figures/param_landscape_heatmaps/microGIF/test_export_2d_heatmap_N_21_loss_tau_theta_J_theta.eps}
%     \vskip -0.1in
%     \caption{p landscape SGIF 3}
% \end{figure}
% Description parameter landscape plots. 


\subsection{Gradient descent with Adam}

To perform optimisation, we used Adam, the adaptive momentum implementation of gradient descent \cite{Kingma2015}.
We note that the spaces for which error gradient given by the firing rate distance is ambiguous, i.e. the gradient may wander somewhat arbitrarily into the space of low to zero error, where it settles, and is thus not satisfactory in terms of defining a signal that will allow for retrieving the ground-truth.
Further, for the van Rossum distance, these landscapes are even more obscure and noisy. Therefore, we have excluded using this metric in our experiments in this work.
However, under more well-defined conditions, where the timing of individual spikes relative to the input signal and other neurons' signals is more informative than misleading, the van Rossum distance should theoretically outperform the firing rate distance.
% ... (plot generation if time)

For each model type, we used the firing rate distance from the target synthetic model outputs to the model being fitted, to optimise its parameters, as previously mentioned until the error gradients were diminishing and thus parameters settling into a local minimum, or for a certain number of training iterations ($t_{iter}=100$.
This was performed for $20$ experiments, for each model class. We then assess the fitted model rates, parameter values, and the loss, and perform NMF analysis on fitted model as well as target model and data, assessing the geodesic similarity of the factorised modules.
% This gives us 

Further, we perform direct neuron-level, i.e. full-scale, model inference of a heterogeneous mixture of stochastic general integrate-and-fire (SGIF) neurons, and find that albeit converging to local minima, the stochastic formulation and the optimisation over the NLL lends itself better to GBO than the surrogate over the LIF-model types when using the firing rate distance.
For all model classes, some parameters have a more prominent effect on the the loss than others, particularly when considering the rate-based loss metric. 
% This is in part handled by using Adam. 
% When considering the NLL, the parameter landscape plots reveal that 
% Illuminating ...


\section{Results}

Results are reported for 20 experiments for each model class, fitted to models of size $N=4$ for LIF and GLIF neurons in this chapter, and synthetically for SGIF models to models of size $N=4$ as well as $N=21$.
These are abbreviated as "meGIF" and "miGIF" respectively - representing meso-GIF and micro-GIF, adopted from \cite{Rene2020}.
The main results are the rates per configuration (i.e. model class and loss function) - implicitly also representing the loss, the parameter distances from the fitted to the ground-truth models, and the geodesic similarities as obtained via NMF.

\subsection{GBO results}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\columnwidth]{figures/samples/SameModelClassTarget/export_spike_trains_euid_12-09_16-17-13-464.eps}
%     \includegraphics[width=0.4\columnwidth]{figures/samples/SameModelClassTarget/export_spike_trains_euid_12-10_09-45-02-423.eps}
%     \caption{Sample LIF and GLIF SNNs after fitting, N=4.}
%     \label{fig:sample_LIF_GLIF_N_4}
% \end{figure}

Included here are two sample figures \ref{fig:sample_GLIF_exp}, \ref{fig:sample_LIF_exp} for particular LIF and GLIF experiments.
As may be glimpsed from these, particularly for the LIF model, we may end up in local minima further away from the ground truth. On average, we end up further from the ground truth for LIF models, and around the same distance for GLIF models, whereas for SGIF models, we end up somewhat closer.
As for the rates, these also reflect these results, with the closes fits being attained for the GLIF and SGIF models.

\begin{figure}
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/samples/GLIF/12-09_16-14-54-627/spike_trains_train_iter_100.png}
    \includegraphics[width=0.49\columnwidth]{figures/samples/GLIF/12-09_16-14-54-627/plot_loss_test12-09_16-16-45-54312-09_16-16-45-543.png}
    \includegraphics[width=0.5\columnwidth]{figures/samples/GLIF/12-09_16-14-54-627/membrane_pots_train_i_100.png}
    \includegraphics[width=0.49\columnwidth]{figures/samples/GLIF/12-09_16-14-54-627/test_weights_inference_trajectories_param_w.png}
    
    \caption{Sample GLIF SNN, spike trains after training for a particular experiment, along with the loss per epoch, corresponding membrane potentials for the spike train plotted above, and the average trajectory of the weights throughout inference.}
    \label{fig:sample_GLIF_exp}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/samples/LIF/12-10_09-55-57-223/spike_trains_train_iter_100.png}
    \includegraphics[width=0.49\columnwidth]{figures/samples/LIF/12-10_09-55-57-223/plot_loss_test12-10_09-56-55-12812-10_09-56-55-128.png}
    \includegraphics[width=0.5\columnwidth]{figures/samples/LIF/12-10_09-55-57-223/membrane_pots_train_i_100.png}
    \includegraphics[width=0.49\columnwidth]{figures/samples/LIF/12-10_09-55-57-223/test_weights_inference_trajectories_param_w.png}
    
    \caption{Sample LIF SNN, spike trains after training for a particular experiment, along with the loss per epoch, corresponding membrane potentials for the spike train plotted above, and the average trajectory of the weights throughout inference.}
    \label{fig:sample_LIF_exp}
\end{figure}

% \subsubsection{Average model rate and parameter distance}

% GBO
\begin{figure}
    \centering
	\includegraphics[width=0.49\columnwidth]{figures/export_rates_saved_all.eps}
	\includegraphics[width=0.49\columnwidth]{figures/export_p_dists_saved_all.eps}
	\caption{Fitted model rates across model types and loss metrics, and average parameter distance between the ground-truth model and both the initial and converged inferred models when using \textbf{GBO} for model inference. see figure \ref{fig:rate_p_dists_SBI} for comparison with SBI.}
	\label{fig:rate_p_dists_GBO}
\end{figure}

Overall, these results show that ground-truth retrieval is highly unlikely with the specified model definitions and loss metrics, as illuminated by the error landscape plots in \ref{sect:e_landscapes}.
% However, they demonstrate that although local minima are inferred, they may be done so by using GBO - the scalable approach recently highly successfully and widely applied ML-algorithm - here for SNNs.
% The results highlight that the procedure is feasible by demonstration a working implementation and integration of various model classes, for both stochastic and leaky integrate-and-fire, and highlight that a stricter definition of the model perturbation scheme, as well as a more informative loss metric design, may be key to improving inference performance.


\subsection{Stochastic general integrate-and-fire neurons}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/samples/SameModelClassTarget/poisson/12-09_17-56-51-235/export_spike_trains_euid_12-09_17-56-51-235.eps}
    \caption{Target and fitted SGIF model spike trains, and loss per training epoch, with the Poisson negative log-likelihood as the loss metric, plotted in figure \ref{fig:PNLL_SGIF_sample_loss}.}
    \label{fig:sample_SGIF_plots}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/samples/SameModelClassTarget/poisson/12-09_17-56-51-235/export_microGIF_plot_loss_euid_12-09_17-56-51-235.eps}
    \caption{The Poisson negative log-likelihood loss per epoch for the experiment with the spike train after training plotted in figure \ref{fig:sample_SGIF_plots}.}
    \label{fig:PNLL_SGIF_sample_loss}
\end{figure}

These results show that GBO is possible for direct, scalable inference of neuron-level models when using a stochastic formulation of the LIF model and the Bernoulli or Poisson NLL by assuming spikes distributed in either of the corresponding probability densities, both being exponential family distributions with shared properties for the case of spike trains.

% SameModelClassTarget
% \subsection{Known ground-truth synthetic data}


\subsubsection{Comparison with published ABC results}

In addition to implementation of GBO for LIF and GLIF SNNs, we test our approach on SGIF neurons, which may lend themselves better to GBO with NLL minimisation over spike probabilities. We here report our comparative results for this model class, as when compared to the work which we adopted the model from \cite{Rene2020}.

\subsubsection{Model input perturbation and formulation}

While we want to model the site from which spikes have been recorded and decoded, we do not (usually) have the input to the site recorded from in biological spike train data.
However, we may make some assumptions about the statistical nature of the input data, and incorporate this into model perturbation. Optionally, we may even design a particular perturbation scheme, such as sinusoidal stimulation, as this may also be performed in vitro.

In any case, in the stochastic general integrate-and-fire model, we assume that the model activity and spike history can be expressed as a set of stochastic equations, which then allows us to optimise over the model spike probabilities for given target spike histories.
In order to test GBO for the SGIF model class, and for comparability with \cite{Rene2020}, we adopt their perturbation scheme, and perturb our models with both sine modulated white noise, and with by using an Ornstein-Uhlenbeck process, and report the results for each case.

\subsubsection*{Sine modulated white noise input}

\begin{equation}
    \sum^M \sin(W_n(t))
\end{equation}

\subsubsection*{White noise input}

\begin{equation}
    W_n(t) \sim \mathcal{U}(t, m)
\end{equation}



\begin{table}
\caption{Neuronal correlations $\rho$ for GBO, converged runs, network size N=4, for the SGIF SNN.}
\label{tab:rho_converged_GBO_pop}
\begin{center}
\begin{tabular}{ l l c c c c c }
 & & \multicolumn{4}{c}{$\rho$ per neuron} \\
 & & $e_{L2/3}$ & $i_{L2/3}$ & $e_{L4}$ & $i_{L4}$ \\
 \textbf{WN} & \textbf{Bernoulli} & 0.09 & 0.14 & 0.21 & 0.07 \\ 
 \textbf{WN} & \textbf{Poisson} & 0.27 & -0.02 & -0.02 & 0.04 \\  
 \textbf{OU} & \textbf{Bernoulli} & 0.23 & 0.35 & 0.14 & 0.14 \\ 
 \textbf{OU} & \textbf{Poisson} & 0.22 & -0.03 & 0.04 & 0.03 \\  
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{RMSE for GBO, converged runs, network size N=4, for the SGIF SNN.}
\label{tab:rho_converged_GBO_pop}
\begin{center}
\begin{tabular}{ l l c c c c c c }
& & \multicolumn{4}{c}{RMSE per neuron} & \textit{Convergence} \\
& & $e_{L2/3}$ & $i_{L2/3}$ & $e_{L4}$ & $i_{L4}$ \\
 \textbf{WN} & \textbf{Bernoulli} & 8.9 & 11.5 & 19.9 & 29.0 & 50 \% \\ 
 \textbf{WN} & \textbf{Poisson} & 11.5 & 14.6 & 19.8 & 25.0 & 80 \% \\  
 \textbf{OU} & \textbf{Bernoulli} & 8.7 & 10.8 & 16.8 & 28.6 & 70 \% \\ 
 \textbf{OU} & \textbf{Poisson} & 11.2 & 13.3 & 17.3 & 25.5 & 85 \% \\  
\end{tabular}
\end{center}
\end{table}

For further results on this model class, including the geodesic NMF module similarity, this is included outwith this subsection, alongside the results for the other model classes.



\subsection{Simulation-based inference}

Using the best performing loss-metric as found in the GBO experiments, and also revealed by the parameter landscape plots, i.e. the firing rate loss metric, we performed SBI by using the Python-framework of the Macke-lab \href{https://github.com/mackelab/sbi}{SBI}.

In sum, SBI over a rate-based metric performed slightly worse for SGIF, and for the leaky integrate-and-fire models was on par for the less high-dimensional LIF model, but performed worse for the more complex GLIF model.
As for retrieving the ground truth parameters, this is where SBI might supplement and/or outperform a GBO approach, as we calculate the full posterior over the model parameters, given the observations.
Interestingly, however, even for the relatively low-dimensional LIF-model, the posteriors are somewhat skewed to the side of the ground-truth values. They do indeed generally give a reasonable estimate for the centers of the pdfs for the parameters for the LIF model, and to some extent for the SGIF model, but fails to do so for the GLIF model.
The declining performance matches well with the increasing number of model parameters, which might be expected with an ABC approach, as posterior approximation requires increasingly more patterns with an increasing dimensionality - and likely even exponentially so.
This shows that particularly for more complex models, which SNNs quite often are, the advantages and applicability of ABC may diminish - rendering GBO if not the only tractable approach, even a competitive wrt parameter inference.

\begin{figure}
    \centering
	\includegraphics[width=0.49\columnwidth]{figures/sbi_plot_rates_all.eps}
	\includegraphics[width=0.49\columnwidth]{figures/sbi_mean_p_dist_all.eps}
	\caption{Fitted model rates across model types and loss metrics, and average parameter distance between the ground-truth model and both the initial and converged inferred models when using \textbf{SBI} for model inference. see figure \ref{fig:rate_p_dists_GBO} for comparison with GBO.}
	\label{fig:rate_p_dists_SBI}
\end{figure}


\begin{figure}
    \centering
	\includegraphics[width=0.5\columnwidth]{figures/sbi_p_avgs_pairplot_SNPE_LIF_12-15_04-56-19-338.eps}
	\caption{Mean posterior marginals between parameters for the LIF model class, fitted using the same ground-truth model and synthetic data as in the LIF GBO experiments.}
\end{figure}

\begin{figure}
    \centering
	\includegraphics[width=\columnwidth]{figures/sbi_p_avgs_pairplot_SNPE_GLIF_12-15_14-22-38-919.eps}
	\caption{Mean posterior marginals between parameters for the GLIF model class.}
\end{figure}

\begin{figure}
    \centering
	\includegraphics[width=0.85\columnwidth]{figures/sbi_p_avgs_pairplot_SNPE_microGIF_12-14_16-14-12-736.eps}
	\caption{Mean posterior marginals between parameters for the SGIF model class.}
\end{figure}



\subsection{NMF analysis}

We analyse the fits attained with the different approaches in order to assess to what extent the inferred models capture functional organisation of the networks that generated the target data.
A high similarity of the factorised representations of neuronal co-activity indicates that the inferred models capture the spatiotemporal behaviour of the target model, which may suggest that the model can be used for further evaluation, such as ing probing to assess functional aspects or in testing or generating functionally related hypotheses. These are more relevant for biological data sets, of course.



% Custom coupled GLM fitted to target spike trains. Baseline.
% citecite

\begin{figure}
    \hspace{-0.1\columnwidth}
    \includegraphics[width=1.2\columnwidth]{figures/matlab/export_GLM_filters_pred_bin_size_0_1_cell_2_target_GT_model_mesoGIF_N_4.eps}
    \caption{A stimulus response filter, coupling filters, and spike train rate predictions for a GLM fitted to a population level (N=4) SGIF model.}
    \label{fig:GLM_mesoGIF}
\end{figure}

As a baseline model for assessing how well we capture higher-order spike statistics, we implement and fit GLMs as previously described, and also perform NMF on the predicted spike trains of these models when perturbing them just as with the other models when generating their spike trains with white noise.
A sample figure of one of the neuron in the GLM is shown in figure \ref{fig:GLM_mesoGIF}.
Due to the fairly low-dimensional nature of the LIF model, MLE for a coupled Poisson GLM fits almost perfectly to the generative target LIF model.
Both SBI and GBO also perform fairly well for this model class in terms of the geodesic similarity (see figure \ref{fig:geodesic_all}).
However, when considering the more complex GLIF model class, which may exert a wider range of spike patterns and behaviours, the GLM performs slightly worse than GBO using a rate-based metric.
Interestingly, it performs just as well for both small and large SGIF networks, as the limit there seems to be the stochasticity introduced in this model class.
It is however outperformed for smaller SGIF networks by both GBO and SBI using NLL minimisation, but performs slightly better than GBO for larger networks.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.1\columnwidth]{figures/matlab/export_GLM_filters_pred_bin_size_0_1_cell_2_target_GT_model_microGIF_N_21.eps}
%     \caption{GLM microGIF}
%     \label{fig:GLM_microGIF}
% \end{figure}


\begin{figure}
    \includegraphics[width=0.49\columnwidth]{figures/matlab/NMF/ACs_target_GT_model_mesoGIF_N_4.eps}
    \includegraphics[width=0.49\columnwidth]{figures/matlab/NMF/ACs_nuovo_spikes_mt_microGIF_euid_12-09_16-02-03-400_lfn_bernoulli_nll.eps}
    \centering
    \includegraphics[width=0.3\columnwidth]{figures/matlab/NMF/target_GT_model_mesoGIF_N_4.eps}
    \includegraphics[width=0.3\columnwidth]{figures/matlab/NMF/modules_nuovo_spikes_mt_microGIF_euid_12-09_16-02-03-400_lfn_bernoulli_nll_4.eps}
    \caption{ACs (top) SGIF, N=4, target (left) fitted (right), Bernoulli NLL, and modules (bottom)}
\end{figure}

\begin{figure}
    \includegraphics[width=0.49\columnwidth]{figures/matlab/NMF/ACs_target_GT_model_microGIF_N_21.eps}
    \includegraphics[width=0.49\columnwidth]{figures/matlab/NMF/ACs_nuovo_synthetic_v2_spikes_mt_microGIF_lfn_bernoulli_nll_euid_01-01_15-56-11-305.eps}
    \caption{ACs SGIF, N=4, target (left) fitted (right), Bernoulli NLL. modules plotted in \ref{fig:modules_SGIF_N_21}}
    \label{fig:ACs_SGIF_N_21}
\end{figure}

\begin{figure}
    \includegraphics[width=0.3\columnwidth]{figures/matlab/NMF/modules_target_GT_model_microGIF_N_21_4.eps}
    \includegraphics[width=0.3\columnwidth]{figures/matlab/NMF/modules_nuovo_synthetic_v2_spikes_mt_microGIF_lfn_bernoulli_nll_euid_01-01_15-40-04-701.eps}
    \caption{NMF modules SGIF, N=21, target (left) fitted (right), Bernoulli NLL, for \ref{fig:ACs_SGIF_N_21}}
    \label{fig:modules_SGIF_N_21}
\end{figure}

\begin{figure}
    \includegraphics[width=0.49\columnwidth]{figures/matlab/NMF/ACs_target_GT_model_mesoGIF_N_4.eps}
    \includegraphics[width=0.49\columnwidth]{figures/matlab/NMF/ACs_target_GT_model_microGIF_N_21.eps}
    \caption{ACs SGIF, N=4, N=21 (right)}
\end{figure}


\begin{figure}
    \hspace{-0.1\columnwidth}
    \includegraphics[width=1.2\columnwidth]{figures/matlab/NMF_geodesic_all_Synthetic_v2.eps}
    \caption{all NMF distances synthetic exps}
    \label{fig:geodesic_all}
\end{figure}


\section{Discussion}

The results showing that for GLIF and SGIF SNNs, the fits are better than for LIF SNNs may be somewhat counter-intuitive, but may be explained by considering that although we introduce greater complexity via the higher number of parameters for the GLIF and SGIF models, their formulations are also more robust in terms of the behaviours they may exhibit.
This may be further emphasised by considering our empirical data from trying to apply GBO to the Izhikevich model, discussed in section \ref{section:izhikevich}, in which we observed that fitting this model was highly challenging due to parameter regions for which model behaviour would be completely chaotic.
This is due to that the model is a lower-dimensional, collapsed projection of a system designed to be able to exhibit a myriad of behaviours (namely the Hodgkin-Huxley \cite{HH1952} model), but it comes at the cost of introducing regions in this projected system for which impossible and chaotic behaviours emerge.

\subsection{Stochasticity and SNNs}

There usually is a larger space for which performance given by the loss metric is fairly equal (as depicted by the error landscape plots), just requiring a different combination of parameters - i.e. there is no single fixed point or trajectory leading to it in the parameter landscape leading to a global minimum.
My interpretation of this is that what we identify with learning algorithms for SNNs is not a specific configuration which captures the data set at hand, but a configuration which reaches a mode of behaviour that would allow it do produce the data.

\subsection*{Multi-layer versus single-layer SNNs}

Multi-layer nets may be crucial for non-linear function approximation. 
This work only considers fully connected "single-layer" SNNs.
Thus, testing the approach for multi-layer SNNs might give new interesting insights, and may also enable more easily capturing more complex data.
Perhaps this is one of the key missing pieces for taking the step from the succesful optimisation of the models when fitting to a lower-dimensional signal, as studied in chapter \ref{chpt:gated_synaptic}, to fitting directly to target spike trains?
% We have not had this. If we only wish to capture the spike statistics, then a multi-layer version training on the next spike given the previous could perform better than the current single-layer noise-input to spike output?
% What we have tried is instantaneous reactivity given noise-input. Is this not largely spatially oriented? To include a more temporal orientation, we may need to redesign the setup.

% Auto-encoding task of own readout would make sense. However, we only have the binary spike output, and not continuous membrane potential


% \subsection{Negative results}

% Optimisers, parameter landscapes, hyperparameters, (linear) parameter constraints, input-output data, model topology and recurrence.

Preliminary conclusion: The spatiotemporal nature of SNNs, with the main measured effect being a function over a highly composite variable that has a high temporal dependency and thus variability, makes optimisation difficult with the current common definitions of optimisation algorithmics, including the loss metrics mentioned in this thesis.
In addition to being highly sensitive to initial conditions and other factors potentially both skewing the times of spiking, as well as the very mode of behaviour and spiking, input-output transformations, and the spatiotemporal signature of spike trains, need to be handled in a way that is more robust to these perturbations. 
Currently, there seems to be no defined loss metric that may capture the distance and resulting parameter landscape when going from one mode of behaviour to another in network nodes in a way that is exploitable by means of gradient based optimisation.

Overall, the results show that ground-truth retrieval is highly unlikely with the specified model definitions and loss metrics, as illuminated by the error landscape plots in \ref{sect:e_landscapes}.
However, they demonstrate that although local minima are inferred, they may be done so by using GBO - the scalable approach recently highly successfully and widely applied ML-algorithm - here for SNNs.
The results highlight that the procedure is feasible by demonstration a working implementation and integration of various model classes, for both stochastic and leaky integrate-and-fire, and highlight that a stricter definition of the model perturbation scheme, as well as a more informative loss metric design, may be key to improving inference performance.


% =======================================================
% =======================================================
% =======================================================
\chapter{Sleep regulation in the rodent brainstem}\label{chpt:sleep}

My initial research proposal outlines a research project where the goal is to meet research needs within the field of sleep regulation through computational modelling.
More specifically, the aspiration was to model neurons of the pedunculopontine and laterodorsal tegmental areas within the brainstem during different brain states, based on their neuroanatomy as described, even though scarcely, within the literature, and based on in vivo data from these brain areas \cite{Herice2019c, Tsunematsu2019, Pal2007, Martinez-Gonzalez2011, Fraigne2015}.
Further, an overarching research goal was to be able to capture the emergence of neural ensembles as identified in vivo in the model by using non-negative matrix factorisation (NMF) \cite{Seung1999, Seung2001, Onken2016a}.
Computational modelling and spiking neural network (SNN) inference covers several research needs within the field of sleep research and the synthesis of neuroscience and its computational counterpart in that it addresses model scarcity, as well as a methodology for accelerating modelling by inference through gradient-based optimisation \cite{Herice2019c, Huh2017, Taherkhani2020}.
This was the foundation for sparking my interest in a methodological project in which we seek to automate biologically relevant neural network model inference, and more specifically to research both (1) the current state-of-the-art on SNN inference, and (2) leveraging ML based methods of gradient descent and optimisation for SNN inference \cite{Huh2017, Mostafa2020, Tavanaei2019b, Lee2016}.

Sleep is widespread across different animal species, crucial to mental functioning. However, why we sleep, and how we sleep, remains to be understood. Some models exist that seek to capture the phasic nature of sleep, but mostly at an abstract level. 
There is as such a need for detailed models in the field, with no current models encompassing direct biological parallels. Addressing the need for modelling in the field, and seeking to illuminate how we sleep, we propose to use a set of recently combined methodologies that allow us to infer neuron-level spiking models based on recorded spike trains. 
In other words, the methodology allows for incorporating both existing knowledge, and to use spike sorted LFP recordings to infer the most statistically probable distributions of model parameters.
This algorithmic approach applied to spike train data has, to the best of my knowledge, not been previously explored. 
This is likely due to the high dimensional parameter search space, which requires a correspondingly complex approach in order to make computation tractable.
The approach requires only partial data - as is always a constraint within neuroscientific recordings. 

While it is evident that the extent to which GBO may be used to infer an exact SNN model based on spike data from the findings in the previous chapter, the results nevertheless show that the approach may infer models that capture the higher-order statistics and functional organisation fairly well, and even better than a solid baseline model, namely the GLM.
As the ABC approach of SNPE quickly becomes intractable, and is in fact intractable for the network size of the biological data studied in this chapter (at least for a reasonable execution time, with limited computational resourced), we have employed our GBO procedure to biological spike train data for LIF, GLIF, and SGIF models, and similarly to in the previous chapter, we assess the goodness of fit via the attained model rates, loss, and geodesic NMF module similarities.


\section{Background}

\subsection{Sleep stages}
Three-process switch model. Different timescales may be at play, and constitute the switching-mechanism.
Can an inferred SNN be used as a starting point to create such a model?

\subsection{PPT/LDT}

Complex circuitry means a myriad of ppt/ldt inputs, some of which are poorly understood.
Could these somehow be reverse-engineered given the state and neural activity, classifying the neurons into types, which can tell us something about the pathways involved, and whether they’re “ON” or “OFF”?
Simple state-driven input as on/off per state to the ensembles should tell us something about each ensemble and its state-preference?
If so, these can be identified, parameters compared and fixed, and then we could try to reconstruct the projections.

% PPT/LDT LFP data; most spiking during NREMS
% (to consider: Little time spent in NREMS)
Want to primarily look at REMS: disinhibition of monoaminergic LC projections (prerequisite; Timofeev et al. (2017)) (shut off / inhibited in LC by GABA), 

PPT/LDT:
Cholinergic are REMS and wake
GABAergic; REMS, wake, or both
NREM?


\section{Analysis of target data}

Data from PPT/LDT area/brainstem. Evaluation and assessment through (rigorous) analysis and classification/prediction of brain state. Shows four well-qualified experiments for further analysis and bridging using the mesoscopic modelling approach.
Provides a solid bridge from the domain of experimental data to generating data using a simulated model that is derived from a good data foundation that we know is well-aligned with brain state.

% Idea: “ultradian” process for synchronous/asynchronous activity -> nREM/REM, also should give rise to qualities during wakefulness (such as better brainstorming/creativity, or some types of productivity in “phases”/stages)

% Idea: Cell types and model parameters that may be used in type classification from fitting data to GLIF models is lacking for the mouse brainstem in the Allen Brain project database.

Replicated results of \cite{Tsunematsu2019}, finding that NMF ACs were more indicative of future brain state than HPC signals, which was a novel finding presented in the original work.
This also means that the data should contain information that if captured in a model may illuminate the functional dynamics associated with sleep regulation in the area (PPT/LDT).

In order to test whether ...
% See first year(?) review
However, single-neuron signals slightly better predictors - suggesting both that there was some information lost in factorisation, but also that the ensembles were very good representations of functional units, as argued in the original paper.

% Insert 1st years' work here. ??

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/LDA/lda_temporal_shifting_and_prediction_bins_4_lda_acs_temporal_windows_4_exp_6.eps}
    \includegraphics[width=0.49\columnwidth]{figures/LDA/lda_temporal_shifting_and_prediction_bins_4_lda_acs_temporal_windows_4_exp_4.eps}
    \caption{Most well-defined data exps: 4 and 6, i.e. rich firing across neurons. best state-prediction performance}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/LDA/bars_LDA_per_signal_t_3.eps}
    \includegraphics[width=0.49\columnwidth]{figures/LDA/bars_RF_per_signal_t_3.eps}
    \caption{LDA \& RF over exps}
\end{figure}

Replicated from \cite{Tsunematsu2019} % double-check ref.
Suggests spike statistics well captured in factorised modules, as ACs sufficient to predict brain state.
Thus, functional ensembles should be well represented by modules, and be a correspondingly suitable methodology to assess capturing these by calculating the similarity of the factorised modules for fitted SNN models.
As the similarity metric, we simply calculate the geodesic similarity between modules, as previously in the synthetic SNN inference work.

\section{Inference}

% ABC too costly.
% \subsection{GBO}
Differentiable SGIF-model adapted from \cite{Rene2020}, using GD for direct neuron-level (microscopic) SNN inference, by negative log-likelihood minimisation, assuming either a Bernoulli or Poisson distribution of the spike train.

GLM baseline

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/sleep/plot1_cell5_pdf.eps}
    \caption{response filters GLM exp 5}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/sleep/GLM_multi_cell5_5sec_bin_white_noise.eps}
    \caption{GLM sample cell 5 exp 5}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/sleep/approx_rate_across_exp_microGIF_bernoulli_nll_vs_fitted.eps}
    \includegraphics[width=0.49\columnwidth]{figures/sleep/approx_rate_across_exp_microGIF_poisson_nll_vs_fitted.eps}
    \caption{inferred model firing rates per data set for the SGIF model, using white noise with a fixed rate as input perturbation, Bernoulli NLL (top), and Poisson NLL (bottom)}
    \label{fig:approx_rates_sleep_exps_SGIF}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/sleep/ACs138.eps}
    \includegraphics[width=0.49\columnwidth]{figures/sleep/ACs_SGIF_exp5_fit_bernoulli_nll_2.eps}
    \caption{ACs exp 5 target (left), and SGIF fit (right) Bernoulli NLL minimisation}
    \label{fig:ACs_exp5}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.3\columnwidth]{figures/sleep/modules138.eps}
    \includegraphics[width=0.3\columnwidth]{figures/sleep/modules_SGIF_exp5_fit_bernoulli_nll.eps}
    \caption{ACs exp 5 target (left), and SGIF fit (right). these particular modules have a geodesic similarity of approximately $73 \%$.}
    \label{fig:modules_exp5}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/sleep/geodesic_exp138.eps}
    \caption{geodesic distances across model types and loss functions for exp. \#5}
    \label{fig:geodesic_distances_exp5}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/sleep/ACs147.eps}
    \includegraphics[width=0.49\columnwidth]{figures/sleep/ACs_nuovo_sleep_v2_spikes_mt_microGIF_euid_12-29_02-12-29-631_exp_6_lfn_poisson_nll.eps}
    \caption{ACs exp 7 target (left), and SGIF fit (right), Poisson NLL minimisation}
    \label{fig:ACs_exp7}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.3\columnwidth]{figures/sleep/modules_exp147.eps}
    \includegraphics[width=0.3\columnwidth]{figures/sleep/modules_nuovo_sleep_v2_spikes_mt_microGIF_euid_12-29_02-12-29-631_exp_6_lfn_poisson_nll.eps}
    \caption{ACs exp 5 target (left), and SGIF fit (right). these particular modules have a geodesic similarity of approximately $69 \%$}
    \label{fig:modules_exp7}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/sleep/geodesic_exp147.eps}
    \caption{geodesic distances across model types and loss functions for exp. \#7}
    \label{fig:geodesic_distances_exp7}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/sleep/geodesic_exp147.eps}
    \caption{geodesic distances across model types and loss functions for exp. \#7}
    \label{fig:geodesic_distances_exp7}
\end{figure}



% {'exp108_microGIF_bernoulli_nll': 11.975849, 'exp108_microGIF_poisson_nll': 11.67591, 'exp109_microGIF_bernoulli_nll': 10.515618, 'exp109_microGIF_poisson_nll': 10.103509, 'exp124_microGIF_bernoulli_nll': 10.443208, 'exp124_microGIF_poisson_nll': 13.823259, 'exp126_microGIF_bernoulli_nll': 11.697008, 'exp126_microGIF_poisson_nll': 13.574955, 'exp138_microGIF_bernoulli_nll': 16.86, 'exp138_microGIF_poisson_nll': 11.835349, 'exp146_microGIF_bernoulli_nll': 10.581168, 'exp146_microGIF_poisson_nll': 11.95316, 'exp147_microGIF_bernoulli_nll': 13.653505, 'exp147_microGIF_poisson_nll': 12.750887}
% {'exp108_microGIF_bernoulli_nll': 6.3071237, 'exp108_microGIF_poisson_nll': 6.146446, 'exp109_microGIF_bernoulli_nll': 4.048536, 'exp109_microGIF_poisson_nll': 2.9935741, 'exp124_microGIF_bernoulli_nll': 4.172781, 'exp124_microGIF_poisson_nll': 12.03365, 'exp126_microGIF_bernoulli_nll': 6.742306, 'exp126_microGIF_poisson_nll': 8.469471, 'exp138_microGIF_bernoulli_nll': 14.084356, 'exp138_microGIF_poisson_nll': 6.122085, 'exp146_microGIF_bernoulli_nll': 3.8953998, 'exp146_microGIF_poisson_nll': 7.132417, 'exp147_microGIF_bernoulli_nll': 5.78849, 'exp147_microGIF_poisson_nll': 5.2616377}
% {'exp108_microGIF_bernoulli_nll': 785.5906, 'exp108_microGIF_poisson_nll': 198.94684, 'exp109_microGIF_bernoulli_nll': 825.34186, 'exp109_microGIF_poisson_nll': 247.29172, 'exp124_microGIF_bernoulli_nll': 921.8936, 'exp124_microGIF_poisson_nll': 263.27893, 'exp126_microGIF_bernoulli_nll': 715.59863, 'exp126_microGIF_poisson_nll': 242.95949, 'exp138_microGIF_bernoulli_nll': 1284.1718, 'exp138_microGIF_poisson_nll': 283.38675, 'exp146_microGIF_bernoulli_nll': 787.7815, 'exp146_microGIF_poisson_nll': 233.002, 'exp147_microGIF_bernoulli_nll': 1030.3088, 'exp147_microGIF_poisson_nll': 251.07625}
% {'exp108_microGIF_bernoulli_nll': 478.35587, 'exp108_microGIF_poisson_nll': 90.61257, 'exp109_microGIF_bernoulli_nll': 568.65466, 'exp109_microGIF_poisson_nll': 115.3953, 'exp124_microGIF_bernoulli_nll': 618.6944, 'exp124_microGIF_poisson_nll': 233.36815, 'exp126_microGIF_bernoulli_nll': 503.43787, 'exp126_microGIF_poisson_nll': 148.58171, 'exp138_microGIF_bernoulli_nll': 1448.3081, 'exp138_microGIF_poisson_nll': 186.98854, 'exp146_microGIF_bernoulli_nll': 537.0402, 'exp146_microGIF_poisson_nll': 136.35039, 'exp147_microGIF_bernoulli_nll': 561.5808, 'exp147_microGIF_poisson_nll': 134.77667}
% {'microGIF': {'bernoulli_nll': [11.975849, 10.515618, 10.443208, 11.697008, 16.86, 10.581168, 13.653505], 'poisson_nll': [11.67591, 10.103509, 13.823259, 13.574955, 11.835349, 11.95316, 12.750887]}}
% {'microGIF': {'bernoulli_nll': [6.3071237, 4.048536, 4.172781, 6.742306, 14.084356, 3.8953998, 5.78849], 'poisson_nll': [6.146446, 2.9935741, 12.03365, 8.469471, 6.122085, 7.132417, 5.2616377]}}
% {'microGIF': {'bernoulli_nll': [785.5906, 825.34186, 921.8936, 715.59863, 1284.1718, 787.7815, 1030.3088], 'poisson_nll': [198.94684, 247.29172, 263.27893, 242.95949, 283.38675, 233.002, 251.07625]}}
% {'microGIF': {'bernoulli_nll': [478.35587, 568.65466, 618.6944, 503.43787, 1448.3081, 537.0402, 561.5808], 'poisson_nll': [90.61257, 115.3953, 233.36815, 148.58171, 186.98854, 136.35039, 134.77667]}}

% print('sleep_data_approx_rates', sleep_data_approx_rates)
% sleep_data_approx_rates [4.5312505, 2.3666666, 6.6166663, 6.727778, 7.861111, 24.842594, 8.857842]
% print('sleep_data_approx_rate_stds', sleep_data_approx_rate_stds)
% sleep_data_approx_rate_stds [9.835535, 2.53673, 4.742714, 6.977707, 6.586351, 35.420063, 11.326365]

Poisson significantly lower loss: Ttest\_relResult(statistic=9.871807340796684, pvalue=6.23389044753519e-05)


\section{Neuroscience parallels for sleep regulation}

Limited what can be hypothesised and/or tested in the inferred models .. ?
Out of scope for now. 
However, would be interesting to design different perturbation schemes, which represent and are thought to correspond to different sleep and stimulus states, and assess to what extent the fitted model outputs change accordingly, and comparatively with the biological data.

% Grace (14)
% Temporal structure ow cholin. Neur.s. can affect downstream
% Chemogenetic could not induce REMS
 
% Initiated in ACh PPT/LDT
 
% Denise originally architect. Costa et al. (16)
% - Main difference is time const. of synn. Response; shorter in orig. paper. Can mimic more rhodent-like state shift
% Funding bbsc
 
% Might apply for only neuro.
% Where is probe inserted – dyeing
 
% Phase-locked activities for delta-waves, too
% P-waves might occur during UP-state
% Phase-locked
% Principle of “computation”?
 
% (Principle derivation?)
% Cell paper information contents between cortex and amygdala in humans and monkey
% “A Tradeoff in the neural code across regions and species” January 2004
% Pryluk et al.
 
% Specific q could be: Role of cholin. Neurons. Temporal structure may be important for REMSG
 
% Izhikevich instead of LIF for Grace
% Once they get REMS – state dependence from cholin. Circuitry too
% Temporal structure might be important


\section{Discussion}

% Direct microscopic model inference tractable for non-dales law compliant models by using GD and Adam over a rate-based metric for (G)LIF models, and over the negative log-likelihood for stochastic models.
% Local minima wrt parameters as shown previously with known GT, however, when considering the NMF ensembles captured through the two approaches,

Scalable, tractable approach, works about as well as GLMs wrt NMF modules similarity.
Provides a starting point for automatic model inference, may accelerate SNN inference research and comp neuro res.

The fact that the error landscapes for the parameters are ambiguous explains why the spike trains are not as correlated in the models with GBO as when using ABC/SBI (as reported by \cite{Rene2020}), as GBO numerically and iteratively here updates parameter values in parallel, thus doing so slightly independently of the other parameters. 
This may suggest that sequential parameter inference is required for better convergence when applying GBO for SNNs in this setting.
However, this takes away the key goal of scalability of the inference algorithm, and would render ABC a better candidate for the job, as we then estimate a full posterior over all parameters dependently - which is also the explanation for a higher correlation and better produced target data by the ABC procedure; as this might be capturing the parameter-dependencies better.
Authors have also reported that reproducing realistic data breaks down when drawing from the inferred posterior independently for each parameter, which we also empirically verified in our experimental setup, by using the aforementioned SBI-framework.


% =======================================================
% =======================================================
% =======================================================
\chapter{Gated subthreshold synaptic currents and continuous target signals}\label{chpt:gated_synaptic}

Non-leaky integrate-and-fire (NLIF) model with subthreshold continuous spike signals defined to sum to 1 inside of an active zone \cite{Huh2017}.
Also tested for LIF models.
We find that indeed exact gradient calculation is not a strictly necessary constraint for optimal autoencoding and general predictive encoding performance - thus, we hypothesise that it is the synapse model in combination with the lower-dimensional and continuous target signal that enables optimisation convergence due to greatly constraining the parameter space - and also in that the signal is far less noisy, also resulting in a more well-defined and traversable parameter landscape.

Fast synapses to $1.5 \si{ms}$ for numerical stability during optimisation, instead of double floating point precision as reported in \cite{Huh2017} as required for convergence, since we found this to be sufficient for convergence in the numerical optimisation procedure, whilst maintaining the same performance as for instantaneously fast synapses to counterbalance the effect of the input perturbation and other, slower synaptic currents.

% With NIF neurons, one could analytically show/solve the systems and show where noise would render the loss function useless. However, unsure how to test this for spike output task. Could we use (Mostafa, 2018) combined with (Huh \& Sejnowski, 2017)? Or at least the latter?
% \section{Gated sub-threshold spike-signal model}
% \section{Study: \textit{NLIF for exact gradient based model inference}}
% \subsection{NLIF definition}

\section{Tasks}

\subsection{Auto-encoding}

\begin{equation}
    \frac{do}{dt} = \frac{-o + i}{\tau_f}
\end{equation}

\subsection{General Predictive Encoding}

\begin{equation}
    \frac{do}{dt} = \frac{- o + i + Ao}{\tau_f}
\end{equation}


\section{Results}

The tasks were replicated first for the NLIF model, and then tested on a leaky model type, by incorporating the synapse model into a LIF SNN.

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/Gating/AutoEncoding/NLIF_sample/plot_loss_test_mt_NLIF_et_AutoEncoding_N_30_titers_200.png}
    \includegraphics[width=0.49\columnwidth]{figures/Gating/AutoEncoding/NLIF_sample/test_plot_outputs_NLIF_seed_23.png}
    \caption{Auto-encoding with NLIF, sample experiment. loss on the left, target and readouts on the right}
    \label{fig:autoencoding_NLIF}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/Gating/AutoEncoding/LIF_sample/plot_loss_test_mt_LIF_et_AutoEncoding_N_30_titers_200.png}
    \includegraphics[width=0.49\columnwidth]{figures/Gating/AutoEncoding/LIF_sample/test_plot_outputs_LIF_seed_25.png}
    \caption{Auto-encoding with LIF, sample experiment. loss on the left, target and readouts on the right}
    \label{fig:autoencoding_LIF}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/Gating/GeneralPredictiveEncoding/NLIF_sample/plot_loss_test_mt_NLIF_et_GeneralPredictiveEncoding_N_30_titers_200.png}
    \includegraphics[width=0.49\columnwidth]{figures/Gating/GeneralPredictiveEncoding/NLIF_sample/test_plot_outputs_NLIF_seed_25.png}
    \caption{General predictive encoding with NLIF, sample experiment. loss on the left, target and readouts on the right}
    \label{fig:general_predictive_NLIF}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/Gating/GeneralPredictiveEncoding/LIF_sample/plot_loss_test_mt_LIF_et_GeneralPredictiveEncoding_N_30_titers_200.png}
    \includegraphics[width=0.49\columnwidth]{figures/Gating/GeneralPredictiveEncoding/LIF_sample/test_plot_outputs_LIF_seed_24.png}
    \caption{General predictive encoding with LIF, sample experiment. loss on the left, target and readouts on the right}
    \label{fig:general_predictive_LIF}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/NLIF/test_export_2d_heatmap_N_4_loss_original_loss_W_fast_W_syn.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/NLIF/test_export_2d_heatmap_N_4_loss_original_loss_W_in_O.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/NLIF/test_export_2d_heatmap_N_4_loss_original_loss_W_in_W_fast.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/NLIF/test_export_2d_heatmap_N_4_loss_original_loss_W_in_W_syn.eps}
    \caption{Parameter landscape for NLIF w autoencoding task}
    \label{fig:p_landscape_NLIF_autoencoding}
\end{figure}
% can insert rate plot too

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/LIF/test_export_2d_heatmap_N_4_loss_original_loss_W_fast_W_syn.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/LIF/test_export_2d_heatmap_N_4_loss_original_loss_W_in_O.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/LIF/test_export_2d_heatmap_N_4_loss_original_loss_W_in_W_fast.eps}
    \includegraphics[width=0.49\columnwidth]{figures/param_landscape_heatmaps/gating/LIF/test_export_2d_heatmap_N_4_loss_original_loss_W_in_W_syn.eps}
    \caption{Parameter landscape for LIF w autoencoding task}
    \label{fig:p_landscape_LIF_autoencoding}
\end{figure}
% can insert rate plot too

\section{Discussion}

Results show that the approach works well also for leaky SNN models, demonstrating that aspects relating to the setup enable successful GBO wrt the task, with the model accurately reproducing the target signal.
However, the parameter configuration that is inferred in order to do so, yet varies.

What does this suggest? What does it illuminate about SNN dynamics? What further tests would it be interesting to do?

analog output versus discrete pulses and latent internal states (??)
research has been impeded by lack of supervised learning algorithms
they present differentiable formulation of SNNs, with exact gradient calculation
simple tasks
rate-based models fail to describe fast dynamics of spike-based computation
difficult to optimise discrete, binary all-or-none signals
other methods circumvent non-differentiability

"Note that the gradient calculation procedure involves multiplication between the presynaptic input source and the postsynaptic adjoint state pv, which is driven by the g ˙ps term: i.e. the product of postsynaptic spike activity and temporal difference of error. This is analogous to reward-modulated spike-time dependent plasticity (STDP) [17]."



% =====================================================
% =====================================================
% =====================================================
\chapter{(Discussion) SNN optimisation requires a well-defined setting}

Optimisation enables in-place inference, but convergence requires a well-defined setting
% Can compute manifolds in which the activity lives and moves
% Functioning: “Carried out by movements along manifolds”
% Activity is modulated by incoming activity, which manipulate manifolds
However, successfully implementing (..) is an engineering challenge we have yet to overcome. 
It is difficult to predict what such an algorithm would need to contain, as most applications of DNNs are very domain specific, and extremely or fully prone to catastrophic forgetting when trained on a new data set.
There is still a fundamental gap between general knowledge acquisition and the specialised kind we see within ML.

“Computationally, we now know that optimization of trajectories gives rise to elegant solutions for very complex motor tasks (Harris and Wolpert, 1998; Todorov and Jordan, 2002; Mordatch et al., 2012). We suggest that cost function optimization occurs much more generally in shaping the internal representations and processes used by the brain. Importantly, we also suggest that this requires the brain to have mechanisms for efficient credit assignment in multilayer and recurrent networks.” - Marblestone et al. (2016). Frontiers in Computational Neuroscience
And it doesn’t matter what specific configuration a network reaches, as long as it reaches “the bottom of the loss curve”, i.e. finds a good/satisfactory solution. → Optimal is any configuration solving the task at hand, and optimal is plural, and diverse. Optimal can be a frontier in high-dimensional space. In Bayesian inference Macke argued we may be able to trace this front by drawing shapes between samples forming probable combinations in parameter-space.

% Can we say something about this Pareto-front with GD?
% When sampling from full posterior dist from a SBI posterior we get realistic data - when independently sampling from marginals we get unrealistic data. Flaw in model, and in assuming parameter independence.

Both of these methodologies get at a reasonable frontier - however, learning in itself in the structures is something different, remaining a mystery.

If only the temporal unfolding of spike events was done according to linear transformations, or in a ...
% --> gating res.
It may be argued that under similar conditions, we may detect things such as spike correlations and phase-locking, which may indicate a clear function of the timing of spiking in information processing - however, it is difficult to design a loss metric that would well capture this in a differentiable manner.
By correlation and number of mutations of the ordered spike train set, we can get closer to this - but such a metric would not be differentiable, nor would even such a metric be able to precisely measure whether one network is functionally equivalent to another.
Part of the difficulty stems from that (1) there are multiple configurations which can produce similar spike patterns, and thus we may hypothesise that there is no "ground-truth" optimal solution per se, and (2) due to the multiple factors of stochasticity we cannot operate with precise spike train comparisons. These two points result in that currently the way of going about to analyse biologically realistic models are closer to the way we analyse recordings from the biological brain. As such, a bridge between inference methodologies such as the ones successfully employed in ML, and the field of computational modelling is needed.
A good starting point for this might be to fit SNNs to more low-dimensional, well-defined target signals, preferably with an informative input, as demonstrated to converge well for both non-leaky as well as leaky integrate-and-fire SNNs in chapter \ref{chpt:gated_synaptic}.

\section{Future work}

The question yet remains how we may ameliorate, or even solve automatic model inference for SNN models, and thereby accelerate computational neuroscience research.
Below is a list of points summarising observations that I made during my research that may be useful to have in mind for future research in the area.

\begin{itemize}
    \item Measure input activity, or model this in a meaningful way.
    \item Work with well-defined input-output transformations.
    \item Also simulate multi-layer networks - although these are then non-linear and less straightforward, they could constrain the state space
    \item Constrain parameter intervals as much as possible using sensible intervals (such as when looking at probable cell type mixtures using the Allen Brain DB)
    \item Perform early population level or average comparisons to determine whether setup is indeed suitable for GBO model inference
    \item Preliminary model results cannot necessarily be verified using approximate Bayesian approaches for SNNs
    \item Consider using non-leaky models to greatly constrain model behaviour and thus facilitate GBO
    \item Using a subthreshold synapse model enables subthreshold continuously differentiable signals over the synaptic currents, allowing for optimisation for instance in "silent" models with subthreshold synaptic currents, and potentially improving optimisation
    \item There needs to be a transformation of the IO which brings the model into a stable regime suitable for GBO
        Linear transformations are good alternatives for this, and can save one for a lot of hand-calibration in order to attain a regime in which GBO is well-defined/does not diverge
\end{itemize}




\bibliographystyle{plain}
\bibliography{references}

%% You can include appendices like this:
% \appendix

\appendix\label{further_results}
\title{Further results}

\begin{figure}
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/sleep/approx_rate_across_exp_LIF_no_cell_types_frd_vs_fitted.eps}
    \includegraphics[width=0.49\columnwidth]{figures/sleep/approx_rate_across_exp_GLIF_no_cell_types_frd_vs_fitted.eps}
    \caption{inferred model firing rates per data set for the (non-Dales compliant) LIF model (left), and GLIF model (right), using white noise with a fixed rate as input perturbation.}
    \label{fig:approx_rates_sleep_exps_LIF}
\end{figure}

\appendix\label{appendix:sample_GLM_code}
\title{Sample GLM code}

% 
% \chapter{First appendix}
% 
% \section{First section}
% 
% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).
% TODO: What is the page limit?

\end{document}
